{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6340765",
   "metadata": {},
   "source": [
    "# **The Stress-Sighting Hypothesis**\n",
    "## A Data-Driven Analysis of Global Events and Reports of the Unknown.\n",
    "\n",
    "**The Stress-Sighting Hypothesis** has the project goal of investigating whether there is a meaningful correlation between the frequency of reported UFO sightings and periods of heightened cultural, political or global stress, using historical event data and publicly reported sightings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54068032",
   "metadata": {},
   "source": [
    "## User Story\n",
    "Alex Holloway is an investigative journalist, known for in-depth features that combine cultural analysis with data storytelling. They work with both independent media outlets and major publishers, seeking to explore how society processes uncertainty — from political unrest to media myths.\n",
    "\n",
    "Alex is planning to write an article on how Global Stress Events impact the number of UFO sightings, and has asked us to conduct our analysis, using the publicly available NUFORC (National UFO Reporting Centre) UFO Sightings dataset found [here](https://www.kaggle.com/datasets/NUFORC/ufo-sightings/data)\n",
    "\n",
    "## Business Requirements\n",
    "In an era shaped by information saturation, political polarisation, and global crises, public perception is increasingly complex and emotionally charged. For journalists, researchers, and communicators, understanding how people respond to uncertainty is as important as the events themselves.\n",
    "\n",
    "This project explores the potential relationship between **reported UFO sightings** and **global stress events**, not to investigate extraterrestrial phenomena, but to examine whether these sightings reflect **underlying patterns of public anxiety, media influence, and cultural tension.**\n",
    "\n",
    "The outcome is a data-driven dashboard designed to support those working at the intersection of **data**, **storytelling**, and **public insight**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bd970",
   "metadata": {},
   "source": [
    "![Alex Holloway – Persona Card](../images/alex_holloway_persona_card.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1b80e",
   "metadata": {},
   "source": [
    "### Alex's Requirements:\n",
    "\n",
    "- **Reveal Patterns**\n",
    "\n",
    "Alex needs to identify correlations between historical periods of stress and spikes in UFO reporting - fast, clearly and without technical issues. \n",
    "\n",
    "- **Narrative Context**\n",
    "\n",
    "They want to explore not just *when* things happened, but *why it matters.* Explanatory text and annotations support deeper storytelling.\n",
    "\n",
    "- **Usable Insights**\n",
    "\n",
    "Our charts and summaries must be easy to extract for use in articles or reports, including explanatory captions and legends.\n",
    "\n",
    "- **Trustworthy Structure**\n",
    "\n",
    "The data pipeline must be transparent, ethical and well-documented to ensure and maintain credibility in their journalistic work.\n",
    "\n",
    "### Value Proposition:\n",
    "Our Dashboard must empower users like Alex to:\n",
    "- Translate complex data into cultural insight\n",
    "- Frame journalistic stories with empirical evidence\n",
    "- Uncover social signals hiding in unconventional data\n",
    "- Offer the audience a grounded perspective on how fear, media, and uncertainty intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31b00e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d0ae1",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "Our Hypotheses for this project are as follows:\n",
    "\n",
    "### **Hypothesis 1:** \n",
    "\n",
    "**There is a positive correlation between the number of glabal stress events in a given year and the number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 2:**\n",
    "\n",
    "**Years with higher total stress severity scores are associated with a greater number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 3:**\n",
    "\n",
    "**Cultural media events, (such as the release of UFO-themed films or television series) correspond with noticeable short-term spikes in reported sightings.**\n",
    "\n",
    "For the sake of brevity, we will not outline our validation approaches here, as this will be covered in a seperate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd704a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d992c",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning\n",
    "\n",
    "In this section we will look to extract our data and give consideration to how we will clean it in order to make it effective for analysis. \n",
    "Our first step is to load our first dataset: *ufo_data_scrubbed.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "283daf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hidde\\AppData\\Local\\Temp\\ipykernel_22392\\3637515765.py:7: DtypeWarning: Columns (5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries and load dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a70b6d",
   "metadata": {},
   "source": [
    "Straight away we get a data type warning advising us that columns 5 & 9 have mixed data types. This is less than ideal, and will cause issues further down the line when we attempt to merge, aggregate or model our data. \n",
    "\n",
    "let's go ahead and check our columns with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35bd4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime',\n",
       " 'city',\n",
       " 'state',\n",
       " 'country',\n",
       " 'shape',\n",
       " 'duration (seconds)',\n",
       " 'duration (hours/min)',\n",
       " 'comments',\n",
       " 'date posted',\n",
       " 'latitude',\n",
       " 'longitude ']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list() # list all columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c5431",
   "metadata": {},
   "source": [
    "Based on the principle of zero-indexing, we can see that our 'duration (seconds)' and 'latitude' columns are likely to be our offenders here. \n",
    "I'll now consult with ChatGPT to suggest code to help identify the problems in our code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92024b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "['2`' '8`' '0.5`']\n",
      "Non-numeric values in latitude:\n",
      "['33q.200088']\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fce55",
   "metadata": {},
   "source": [
    "We can see from the code output that we have some uexpected, non-numeric characters populating several rows. \n",
    "Let's now convert these columns to strictly numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45867502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration (seconds)'] = pd.to_numeric(df['duration (seconds)'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "# coerce will convert non-numeric values to NaN\n",
    "# Code provided by ChatGPT to convert columns to numeric types, handling non-numeric values by converting them to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c65f5",
   "metadata": {},
   "source": [
    "Now, let's run our Helper Function again to check that things have been resolved as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ef2deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "[nan]\n",
      "Non-numeric values in latitude:\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d2002",
   "metadata": {},
   "source": [
    "We can now see that we have replaced the non-numeric values with the NaN (Not a Number) value.\n",
    "Let us now flag the number or rows to be dropped, and export the dropped rows to a new .csv file for the purposes of auditing and transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39268af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag rows with invalid (non-numeric) duration or latitude\n",
    "df['invalid_duration_or_latitude'] = df[['duration (seconds)', 'latitude']].isnull().any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37846883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to be dropped: 4\n"
     ]
    }
   ],
   "source": [
    "# Count and optionally save them\n",
    "dropped_rows = df[df['invalid_duration_or_latitude']]\n",
    "print(f\"Number of rows to be dropped: {len(dropped_rows)}\")\n",
    "\n",
    "# Export dropped rows for audit\n",
    "dropped_rows.to_csv(\"../data/dropped_invalid_coordinates_or_duration.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf9d6f",
   "metadata": {},
   "source": [
    "As we can see, there are only 4 rows flagged to be dropped here, which represents ~0.005% of our total data, so let's go ahead and drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35b88476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with invalid duration or latitude\n",
    "df = df[~df['invalid_duration_or_latitude']].drop(columns='invalid_duration_or_latitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479475d",
   "metadata": {},
   "source": [
    "Now let us check that our NaN values have been dropped from the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cf27cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (seconds)    0\n",
      "latitude              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['duration (seconds)', 'latitude']].isnull().sum())\n",
    "# check that there are no more NaN values in the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5f0b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27822</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43782</th>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.440663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration (seconds)   latitude\n",
       "27822                 NaN  33.932500\n",
       "35692                 NaN  36.974167\n",
       "43782               180.0        NaN\n",
       "58591                 NaN   4.440663"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few of the previously dropped values\n",
    "dropped_rows[['duration (seconds)', 'latitude']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9042070",
   "metadata": {},
   "source": [
    "Here we can see that we have successfully removed the rows with NaN values, and that, as expected there are only 4 rows removed.\n",
    "These rows represent such a small fraction of the data (~0.005%) that their absence will not introduce bias, distort correlations, or meaningfully affect the outcome of any regression or visual insights. Removing them ensures a cleaner, more reliable dataset without sacrificing representativeness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77f849",
   "metadata": {},
   "source": [
    "Now that we have solved our initial issue of unexpected characters appearing in the *duration (seconds)* column and the *latitude* column, let us now continue by conducting a broad audit of missing data across the entire dataset. We can conduct a very simple operation here by using a combination of the 'isnull()' and 'sum()' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c5b436d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                   5796\n",
       "country                 9668\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae66e6",
   "metadata": {},
   "source": [
    "We can see here that despite using the 'scrubbed' version of our UFO data, we still have a lot of missing values to deal with. \n",
    "Let us quickly calculate the percentage of the whole database that has the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3824d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                 12.04\n",
       "state                    7.22\n",
       "shape                    2.40\n",
       "comments                 0.02\n",
       "datetime                 0.00\n",
       "city                     0.00\n",
       "duration (seconds)       0.00\n",
       "duration (hours/min)     0.00\n",
       "date posted              0.00\n",
       "latitude                 0.00\n",
       "longitude                0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show percentage of missing values\n",
    "(df.isnull().sum() / len(df) * 100).round(2).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172be28",
   "metadata": {},
   "source": [
    "We can see from this that missing *country* values make up ~12% of our dataset. Missing *state* entries account for ~7.2%.\n",
    "Missing *shape* decriptors accoount for only 2.4%, and missing *comments* only 0.02%.\n",
    "\n",
    "In this instance, let us first turn our attention to resolving the missing *country* values. Due to the statistically significant proportion of our dataset that this represents, we decide to impute the missing values with \"Unknown\" rather than deleting the rows entirely. \n",
    "\n",
    "We can perform this operation using the following methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "676c8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing country values with 'unknown'\n",
    "df['country'] = df['country'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597677a4",
   "metadata": {},
   "source": [
    "let us check that this has worked as anticipating by running our previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bfda9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                   5796\n",
       "country                    0\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd574e",
   "metadata": {},
   "source": [
    "Great! We can see that our *country* column now has zero missing entries, so our imputation has been successful.\n",
    "We can now perform the same operation on the *state* columns. We've decided to take this course of action due to the dataset containing sightings that have occurred in regions outside of the US, and so may not require or have *state* values. These rows may still have relevance to our regional breakdown analysis that we may conduct later on. \n",
    "Let us perform the same operation as before, but alter our code to point to the *state* column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d2da068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing state values with 'unknown'\n",
    "df['state'] = df['state'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761e722",
   "metadata": {},
   "source": [
    "Again, let us run our test to ensure that our operation has been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc788534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                      0\n",
       "country                    0\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde60f03",
   "metadata": {},
   "source": [
    "Success! Let us now move on to addressing the missing values in the *shape* column. Once again, it seems prudent for us to impute 'unknown' values into the missing values here; due to this column potentially feeding into later visulaisations factoring shape type as a notable interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f90d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing shape values with 'unknown'\n",
    "df['shape'] = df['shape'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d423ce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 0\n",
       "city                     0\n",
       "state                    0\n",
       "country                  0\n",
       "shape                    0\n",
       "duration (seconds)       0\n",
       "duration (hours/min)     0\n",
       "comments                15\n",
       "date posted              0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e12d8",
   "metadata": {},
   "source": [
    "We have now solved the majority of our missing values, with only the missing *comments* values remaining. We have two options for resolving this issue. Either we could fill this missing entries with empty string values, or drop the rows entirely. \n",
    "As we saw earlier, these missing values only account for 0.02% of our data, and so because of the low significance to our overall analysis, we decide to drop these rows. \n",
    "For this we use the *dropna()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5cca095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing comments\n",
    "df = df.dropna(subset=['comments'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa9d2351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                0\n",
       "city                    0\n",
       "state                   0\n",
       "country                 0\n",
       "shape                   0\n",
       "duration (seconds)      0\n",
       "duration (hours/min)    0\n",
       "comments                0\n",
       "date posted             0\n",
       "latitude                0\n",
       "longitude               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d13774",
   "metadata": {},
   "source": [
    "Now that we have successfully handled our missing data entries, let us summarise our handling decisions:\n",
    "\n",
    "- *country* : ~12% missing data filled with 'unknown'\n",
    "- *state* : ~7.2% mnissing data filled with 'unknown'\n",
    "- *shape* : ~2.4% missing data filled with 'unknown'\n",
    "- *comments* : ~0.02% missing data dropped. \n",
    "\n",
    "These decisions were made in order to preserve the maximum data integrity, while allowing us flexibility in filtering and consistent formatting for categorical fields for visual analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cac8ea",
   "metadata": {},
   "source": [
    "Next, let us quickly ensure that our column names are standardised, due to us seeing that some columns contain spaces, for example the *duration (seconds)* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f50317f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'city', 'state', 'country', 'shape', 'duration_(seconds)',\n",
       "       'duration_(hours/min)', 'comments', 'date_posted', 'latitude',\n",
       "       'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace spaces in column names with underscores and convert to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222acb8",
   "metadata": {},
   "source": [
    "We can now see that our column names have been standardised, and that we have solved the issues with column names having spaces. This will help us later on when we come to merge our datasets, and also for when we start to conduct our analysis. \n",
    "\n",
    "Next, we should check that our columns are correctly assigned the proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b011d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 object\n",
       "city                     object\n",
       "state                    object\n",
       "country                  object\n",
       "shape                    object\n",
       "duration_(seconds)      float64\n",
       "duration_(hours/min)     object\n",
       "comments                 object\n",
       "date_posted              object\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types of the columns\n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24f5d5",
   "metadata": {},
   "source": [
    "We can see from our data types check that there are some columns that will need to have their data types changed. \n",
    "First on our 'to-do' list is handling the *datetime* column - from 'object' to 'datetime.' We will do this by utilising the *.to_datetime()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4be02b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                datetime64[ns]\n",
       "city                            object\n",
       "state                           object\n",
       "country                         object\n",
       "shape                           object\n",
       "duration_(seconds)             float64\n",
       "duration_(hours/min)            object\n",
       "comments                        object\n",
       "date_posted                     object\n",
       "latitude                       float64\n",
       "longitude                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'datetime' column to datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "#  add the .dtypes function to check the data types again  \n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a9d71",
   "metadata": {},
   "source": [
    "Now that we have converted our *datetime* column to the correct format, let us move on to handling the next column with the incorrect data type - *duration_(hours/min)*\n",
    "\n",
    "Addressing the requirements of this column tells us that it is not needed for our analysis, due to it having inconsistent formatting throughout, so would require some serious, time-consuming parsing. As we also have a *duration_(seconds)* column, we feel that the *duration_(hours/mins)* column is redundant for our purposes. \n",
    "\n",
    "Let us proceed to drop this column from our dataset using the *.drop()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e20fe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'duration_(hours/min)' column as it is redundant\n",
    "df.drop(columns=['duration_(hours/min)'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e20d2",
   "metadata": {},
   "source": [
    "Now that we have removed the column, let us quickly check that it has been successfully removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "55dfa424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  city    state  country     shape  \\\n",
       "0 1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1 1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2 1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3 1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4 1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "\n",
       "   duration_(seconds)                                           comments  \\\n",
       "0              2700.0  This event took place in early fall around 194...   \n",
       "1              7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                20.0  My older brother and twin sister were leaving ...   \n",
       "4               900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "\n",
       "  date_posted   latitude   longitude  \n",
       "0   4/27/2004  29.883056  -97.941111  \n",
       "1  12/16/2005  29.384210  -98.581082  \n",
       "2   1/21/2008  53.200000   -2.916667  \n",
       "3   1/17/2004  28.978333  -96.645833  \n",
       "4   1/22/2004  21.418056 -157.803611  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # Display the first few rows of the cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc4b48",
   "metadata": {},
   "source": [
    "We can see that our *duration_(hours/min) column has been successfully removed. \n",
    "Next, let us convert the *date_posted* column into from an 'object' to the correct 'datetime' format. We will utilise the same code as we used before, merely pointing the code to our chosen column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5328e974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'date_posted' column to datetime type\n",
    "df['date_posted'] = pd.to_datetime(df['date_posted'], errors='coerce')\n",
    "#  add the .dtypes function to check the data types again  \n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec022313",
   "metadata": {},
   "source": [
    "We can now see that our columns are now correctly reformatted to their correct types. \n",
    "The next logical step is to check to see if there are any duplicate entries in our dataset. For this, we will emply the use of the .duplicated() and .sum() methods to show us a number of duplicate entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ffed7b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 2\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (excluding index)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee935d8",
   "metadata": {},
   "source": [
    "Let us check the duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3b9acc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62690</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime       city state country  shape  duration_(seconds)  \\\n",
       "62690 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "70780 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "\n",
       "                 comments date_posted   latitude  longitude  \n",
       "62690   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "70780  2 bright lights...  2013-09-09  38.811944 -77.636667  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show actual duplicate rows\n",
    "df[df.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3e0ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62689</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62690</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70779</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime       city state country  shape  duration_(seconds)  \\\n",
       "62689 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "62690 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "70779 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "70780 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "\n",
       "                 comments date_posted   latitude  longitude  \n",
       "62689   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "62690   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "70779  2 bright lights...  2013-09-09  38.811944 -77.636667  \n",
       "70780  2 bright lights...  2013-09-09  38.811944 -77.636667  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(keep=False)]  # Show all duplicates, including the first occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb03ccd",
   "metadata": {},
   "source": [
    "We can see from our duplicate check, that they have both been duplicated on successive rows after their first entries, and that all information is identically duplicated. We consider it safe therefore, to go ahead and drop these rows from the dataset.\n",
    "For this we will go ahead and employ the *.drop_duplicates function. We'll also make certain to set our argument *inplace=True* to remove them completely. Removing these ensures clean aggregation and avoids skewing any yearly totals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b6a0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeed658",
   "metadata": {},
   "source": [
    "Let us perform a check to ensure that we have removed our 2 duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ade39bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (excluding index)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b69cb",
   "metadata": {},
   "source": [
    "Excellent, we now have handled our duplicated entries successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99d9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
