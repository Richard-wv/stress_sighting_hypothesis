{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6340765",
   "metadata": {},
   "source": [
    "# **The Stress-Sighting Hypothesis**\n",
    "## A Data-Driven Analysis of Global Events and Reports of the Unknown.\n",
    "\n",
    "**The Stress-Sighting Hypothesis** has the project goal of investigating whether there is a meaningful correlation between the frequency of reported UFO sightings and periods of heightened cultural, political or global stress, using historical event data and publicly reported sightings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54068032",
   "metadata": {},
   "source": [
    "## User Story\n",
    "Alex Holloway is an investigative journalist, known for in-depth features that combine cultural analysis with data storytelling. They work with both independent media outlets and major publishers, seeking to explore how society processes uncertainty — from political unrest to media myths.\n",
    "\n",
    "Alex is planning to write an article on how Global Stress Events impact the number of UFO sightings, and has asked us to conduct our analysis, using the publicly available NUFORC (National UFO Reporting Centre) UFO Sightings dataset found [here](https://www.kaggle.com/datasets/NUFORC/ufo-sightings/data)\n",
    "\n",
    "## Business Requirements\n",
    "In an era shaped by information saturation, political polarisation, and global crises, public perception is increasingly complex and emotionally charged. For journalists, researchers, and communicators, understanding how people respond to uncertainty is as important as the events themselves.\n",
    "\n",
    "This project explores the potential relationship between **reported UFO sightings** and **global stress events**, not to investigate extraterrestrial phenomena, but to examine whether these sightings reflect **underlying patterns of public anxiety, media influence, and cultural tension.**\n",
    "\n",
    "The outcome is a data-driven dashboard designed to support those working at the intersection of **data**, **storytelling**, and **public insight**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bd970",
   "metadata": {},
   "source": [
    "![Alex Holloway – Persona Card](../images/alex_holloway_persona_card.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1b80e",
   "metadata": {},
   "source": [
    "### Alex's Requirements:\n",
    "\n",
    "- **Reveal Patterns**\n",
    "\n",
    "Alex needs to identify correlations between historical periods of stress and spikes in UFO reporting - fast, clearly and without technical issues. \n",
    "\n",
    "- **Narrative Context**\n",
    "\n",
    "They want to explore not just *when* things happened, but *why it matters.* Explanatory text and annotations support deeper storytelling.\n",
    "\n",
    "- **Usable Insights**\n",
    "\n",
    "Our charts and summaries must be easy to extract for use in articles or reports, including explanatory captions and legends.\n",
    "\n",
    "- **Trustworthy Structure**\n",
    "\n",
    "The data pipeline must be transparent, ethical and well-documented to ensure and maintain credibility in their journalistic work.\n",
    "\n",
    "### Value Proposition:\n",
    "Our Dashboard must empower users like Alex to:\n",
    "- Translate complex data into cultural insight\n",
    "- Frame journalistic stories with empirical evidence\n",
    "- Uncover social signals hiding in unconventional data\n",
    "- Offer the audience a grounded perspective on how fear, media, and uncertainty intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31b00e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d0ae1",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "Our Hypotheses for this project are as follows:\n",
    "\n",
    "### **Hypothesis 1:** \n",
    "\n",
    "**There is a positive correlation between the number of glabal stress events in a given year and the number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 2:**\n",
    "\n",
    "**Years with higher total stress severity scores are associated with a greater number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 3:**\n",
    "\n",
    "**Cultural media events, (such as the release of UFO-themed films or television series) correspond with noticeable short-term spikes in reported sightings.**\n",
    "\n",
    "For the sake of brevity, we will not outline our validation approaches here, as this will be covered in a seperate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd704a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d992c",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning\n",
    "\n",
    "In this section we will look to extract our data and give consideration to how we will clean it in order to make it effective for analysis. \n",
    "Our first step is to load our first dataset: *ufo_data_scrubbed.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283daf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hidde\\AppData\\Local\\Temp\\ipykernel_21784\\3637515765.py:7: DtypeWarning: Columns (5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries and load dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a70b6d",
   "metadata": {},
   "source": [
    "Straight away we get a data type warning advising us that columns 5 & 9 have mixed data types. This is less than ideal, and will cause issues further down the line when we attempt to merge, aggregate or model our data. \n",
    "\n",
    "let's go ahead and check our columns with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35bd4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime',\n",
       " 'city',\n",
       " 'state',\n",
       " 'country',\n",
       " 'shape',\n",
       " 'duration (seconds)',\n",
       " 'duration (hours/min)',\n",
       " 'comments',\n",
       " 'date posted',\n",
       " 'latitude',\n",
       " 'longitude ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list() # list all columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c5431",
   "metadata": {},
   "source": [
    "Based on the principle of zero-indexing, we can see that our 'duration (seconds)' and 'latitude' columns are likely to be our offenders here. \n",
    "I'll now consult with ChatGPT to suggest code to help identify the problems in our code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92024b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "['2`' '8`' '0.5`']\n",
      "Non-numeric values in latitude:\n",
      "['33q.200088']\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fce55",
   "metadata": {},
   "source": [
    "We can see from the code output that we have some uexpected, non-numeric characters populating several rows. \n",
    "Let's now convert these columns to strictly numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45867502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration (seconds)'] = pd.to_numeric(df['duration (seconds)'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "# coerce will convert non-numeric values to NaN\n",
    "# Code provided by ChatGPT to convert columns to numeric types, handling non-numeric values by converting them to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c65f5",
   "metadata": {},
   "source": [
    "Now, let's run our Helper Function again to check that things have been resolved as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef2deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "[nan]\n",
      "Non-numeric values in latitude:\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d2002",
   "metadata": {},
   "source": [
    "We can now see that we have replaced the non-numeric values with the NaN (Not a Number) value.\n",
    "Let us now flag the number or rows to be dropped, and export the dropped rows to a new .csv file for the purposes of auditing and transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39268af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag rows with invalid (non-numeric) duration or latitude\n",
    "df['invalid_duration_or_latitude'] = df[['duration (seconds)', 'latitude']].isnull().any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37846883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to be dropped: 4\n"
     ]
    }
   ],
   "source": [
    "# Count and optionally save them\n",
    "dropped_rows = df[df['invalid_duration_or_latitude']]\n",
    "print(f\"Number of rows to be dropped: {len(dropped_rows)}\")\n",
    "\n",
    "# Export dropped rows for audit\n",
    "dropped_rows.to_csv(\"../data/dropped_invalid_coordinates_or_duration.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf9d6f",
   "metadata": {},
   "source": [
    "As we can see, there are only 4 rows flagged to be dropped here, which represents ~0.005% of our total data, so let's go ahead and drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b88476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with invalid duration or latitude\n",
    "df = df[~df['invalid_duration_or_latitude']].drop(columns='invalid_duration_or_latitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479475d",
   "metadata": {},
   "source": [
    "Now let us check that our NaN values have been dropped from the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf27cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (seconds)    0\n",
      "latitude              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['duration (seconds)', 'latitude']].isnull().sum())\n",
    "# check that there are no more NaN values in the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f0b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27822</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43782</th>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.440663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration (seconds)   latitude\n",
       "27822                 NaN  33.932500\n",
       "35692                 NaN  36.974167\n",
       "43782               180.0        NaN\n",
       "58591                 NaN   4.440663"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few of the previously dropped values\n",
    "dropped_rows[['duration (seconds)', 'latitude']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9042070",
   "metadata": {},
   "source": [
    "Here we can see that we have successfully removed the rows with NaN values, and that, as expected there are only 4 rows removed.\n",
    "These rows represent such a small fraction of the data (~0.005%) that their absence will not introduce bias, distort correlations, or meaningfully affect the outcome of any regression or visual insights. Removing them ensures a cleaner, more reliable dataset without sacrificing representativeness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77f849",
   "metadata": {},
   "source": [
    "Now that we have solved our initial issue of unexpected characters appearing in the *duration (seconds)* column and the *latitude* column, let us now continue by conducting a broad audit of missing data across the entire dataset. We can conduct a very simple operation here by using a combination of the 'isnull()' and 'sum()' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5b436d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                   5796\n",
       "country                 9668\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae66e6",
   "metadata": {},
   "source": [
    "We can see here that despite using the 'scrubbed' version of our UFO data, we still have a lot of missing values to deal with. \n",
    "Let us quickly calculate the percentage of the whole database that has the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3824d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                 12.04\n",
       "state                    7.22\n",
       "shape                    2.40\n",
       "comments                 0.02\n",
       "datetime                 0.00\n",
       "city                     0.00\n",
       "duration (seconds)       0.00\n",
       "duration (hours/min)     0.00\n",
       "date posted              0.00\n",
       "latitude                 0.00\n",
       "longitude                0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show percentage of missing values\n",
    "(df.isnull().sum() / len(df) * 100).round(2).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172be28",
   "metadata": {},
   "source": [
    "We can see from this that missing *country* values make up ~12% of our dataset. Missing *state* entries account for ~7.2%.\n",
    "Missing *shape* decriptors accoount for only 2.4%, and missing *comments* only 0.02%.\n",
    "\n",
    "In this instance, let us first turn our attention to resolving the missing *country* values. Due to the statistically significant proportion of our dataset that this represents, we decide to impute the missing values with \"Unknown\" rather than deleting the rows entirely. \n",
    "\n",
    "We can perform this operation using the following methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676c8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing country values with 'unknown'\n",
    "df['country'] = df['country'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597677a4",
   "metadata": {},
   "source": [
    "let us check that this has worked as anticipating by running our previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfda9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                   5796\n",
       "country                    0\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd574e",
   "metadata": {},
   "source": [
    "Great! We can see that our *country* column now has zero missing entries, so our imputation has been successful.\n",
    "We can now perform the same operation on the *state* columns. We've decided to take this course of action due to the dataset containing sightings that have occurred in regions outside of the US, and so may not require or have *state* values. These rows may still have relevance to our regional breakdown analysis that we may conduct later on. \n",
    "Let us perform the same operation as before, but alter our code to point to the *state* column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2da068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing state values with 'unknown'\n",
    "df['state'] = df['state'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761e722",
   "metadata": {},
   "source": [
    "Again, let us run our test to ensure that our operation has been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc788534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                      0\n",
       "country                    0\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde60f03",
   "metadata": {},
   "source": [
    "Success! Let us now move on to addressing the missing values in the *shape* column. Once again, it seems prudent for us to impute 'unknown' values into the missing values here; due to this column potentially feeding into later visulaisations factoring shape type as a notable interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f90d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing shape values with 'unknown'\n",
    "df['shape'] = df['shape'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d423ce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 0\n",
       "city                     0\n",
       "state                    0\n",
       "country                  0\n",
       "shape                    0\n",
       "duration (seconds)       0\n",
       "duration (hours/min)     0\n",
       "comments                15\n",
       "date posted              0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e12d8",
   "metadata": {},
   "source": [
    "We have now solved the majority of our missing values, with only the missing *comments* values remaining. We have two options for resolving this issue. Either we could fill this missing entries with empty string values, or drop the rows entirely. \n",
    "As we saw earlier, these missing values only account for 0.02% of our data, and so because of the low significance to our overall analysis, we decide to drop these rows. \n",
    "For this we use the *dropna()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cca095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing comments\n",
    "df = df.dropna(subset=['comments'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9d2351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                0\n",
       "city                    0\n",
       "state                   0\n",
       "country                 0\n",
       "shape                   0\n",
       "duration (seconds)      0\n",
       "duration (hours/min)    0\n",
       "comments                0\n",
       "date posted             0\n",
       "latitude                0\n",
       "longitude               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d13774",
   "metadata": {},
   "source": [
    "Now that we have successfully handled our missing data entries, let us summarise our handling decisions:\n",
    "\n",
    "- *country* : ~12% missing data filled with 'unknown'\n",
    "- *state* : ~7.2% missing data filled with 'unknown'\n",
    "- *shape* : ~2.4% missing data filled with 'unknown'\n",
    "- *comments* : ~0.02% missing data dropped. \n",
    "\n",
    "These decisions were made in order to preserve the maximum data integrity, while allowing us flexibility in filtering and consistent formatting in categorical fields for visual analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cac8ea",
   "metadata": {},
   "source": [
    "Next, let us quickly ensure that our column names are standardised, due to us seeing that some columns contain spaces, for example the *duration (seconds)* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50317f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'city', 'state', 'country', 'shape', 'duration_(seconds)',\n",
       "       'duration_(hours/min)', 'comments', 'date_posted', 'latitude',\n",
       "       'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace spaces in column names with underscores and convert to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222acb8",
   "metadata": {},
   "source": [
    "We can now see that our column names have been standardised, and that we have solved the issues with column names having spaces. This will help us later on when we come to merge our datasets, and also for when we start to conduct our analysis. \n",
    "\n",
    "Next, we should check that our columns are correctly assigned the proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b011d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 object\n",
       "city                     object\n",
       "state                    object\n",
       "country                  object\n",
       "shape                    object\n",
       "duration_(seconds)      float64\n",
       "duration_(hours/min)     object\n",
       "comments                 object\n",
       "date_posted              object\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types of the columns\n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24f5d5",
   "metadata": {},
   "source": [
    "We can see from our data types check that there are some columns that will need to have their data types changed. \n",
    "First on our 'to-do' list is handling the *datetime* column - from 'object' to 'datetime.' We will do this by utilising the *.to_datetime()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4be02b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                datetime64[ns]\n",
       "city                            object\n",
       "state                           object\n",
       "country                         object\n",
       "shape                           object\n",
       "duration_(seconds)             float64\n",
       "duration_(hours/min)            object\n",
       "comments                        object\n",
       "date_posted                     object\n",
       "latitude                       float64\n",
       "longitude                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'datetime' column to datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "#  add the .dtypes function to check the data types again  \n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a9d71",
   "metadata": {},
   "source": [
    "Now that we have converted our *datetime* column to the correct format, let us move on to handling the next column with the incorrect data type - *duration_(hours/min)*\n",
    "\n",
    "Addressing the requirements of this column tells us that it is not needed for our analysis, due to it having inconsistent formatting throughout, so would require some serious, time-consuming parsing. As we also have a *duration_(seconds)* column, we feel that the *duration_(hours/mins)* column is redundant for our purposes. \n",
    "\n",
    "Let us proceed to drop this column from our dataset using the *.drop()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e20fe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'duration_(hours/min)' column as it is redundant\n",
    "df.drop(columns=['duration_(hours/min)'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e20d2",
   "metadata": {},
   "source": [
    "Now that we have removed the column, let us quickly check that it has been successfully removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55dfa424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  city    state  country     shape  \\\n",
       "0 1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1 1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2 1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3 1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4 1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "\n",
       "   duration_(seconds)                                           comments  \\\n",
       "0              2700.0  This event took place in early fall around 194...   \n",
       "1              7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                20.0  My older brother and twin sister were leaving ...   \n",
       "4               900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "\n",
       "  date_posted   latitude   longitude  \n",
       "0   4/27/2004  29.883056  -97.941111  \n",
       "1  12/16/2005  29.384210  -98.581082  \n",
       "2   1/21/2008  53.200000   -2.916667  \n",
       "3   1/17/2004  28.978333  -96.645833  \n",
       "4   1/22/2004  21.418056 -157.803611  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # Display the first few rows of the cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc4b48",
   "metadata": {},
   "source": [
    "We can see that our *duration_(hours/min) column has been successfully removed. \n",
    "Next, let us convert the *date_posted* column into from an 'object' to the correct 'datetime' format. We will utilise the same code as we used before, merely pointing the code to our chosen column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5328e974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'date_posted' column to datetime type\n",
    "df['date_posted'] = pd.to_datetime(df['date_posted'], errors='coerce')\n",
    "#  add the .dtypes function to check the data types again  \n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec022313",
   "metadata": {},
   "source": [
    "We can now see that our columns are now correctly reformatted to their correct types. \n",
    "The next logical step is to check to see if there are any duplicate entries in our dataset. For this, we will emply the use of the .duplicated() and .sum() methods to show us a number of duplicate entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffed7b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 2\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (excluding index)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee935d8",
   "metadata": {},
   "source": [
    "Let us check the duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3b9acc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62690</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime       city state country  shape  duration_(seconds)  \\\n",
       "62690 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "70780 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "\n",
       "                 comments date_posted   latitude  longitude  \n",
       "62690   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "70780  2 bright lights...  2013-09-09  38.811944 -77.636667  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show actual duplicate rows\n",
    "df[df.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3e0ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62689</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62690</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70779</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime       city state country  shape  duration_(seconds)  \\\n",
       "62689 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "62690 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "70779 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "70780 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "\n",
       "                 comments date_posted   latitude  longitude  \n",
       "62689   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "62690   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "70779  2 bright lights...  2013-09-09  38.811944 -77.636667  \n",
       "70780  2 bright lights...  2013-09-09  38.811944 -77.636667  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(keep=False)]  # Show all duplicates, including the first occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb03ccd",
   "metadata": {},
   "source": [
    "We can see from our duplicate check, that they have both been duplicated on successive rows after their first entries, and that all information is identically duplicated. We consider it safe therefore, to go ahead and drop these rows from the dataset.\n",
    "For this we will go ahead and employ the *.drop_duplicates function. We'll also make certain to set our argument *inplace=True* to remove them completely. Removing these ensures clean aggregation and avoids skewing any yearly totals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b6a0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeed658",
   "metadata": {},
   "source": [
    "Let us perform a check to ensure that we have removed our 2 duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ade39bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (excluding index)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b69cb",
   "metadata": {},
   "source": [
    "Excellent, we now have handled our duplicated entries successfully. \n",
    "Next, we think that it would be prudent to standardise values in our key categorical fields. By this we mean, normalise the descriptors into having the same formatting across the entire dataset e.g. 'USA', 'usa' and 'Usa' all mean the same, but could be interpreted as being separate in our analysis. The columns that we want to perform this action against are, *city*, *state*, *country* and *shape*.\n",
    "\n",
    "We will create a 'for loop' to convert our text fields into lowercase, and strip any whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d99d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text fields to lowercase and strip whitespace\n",
    "for col in ['city', 'state', 'country', 'shape']:\n",
    "    df[col] = df[col].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d30095",
   "metadata": {},
   "source": [
    "Let us also quickly check to make sure that our *longitude* and *latitude* values all fall within expected ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04431061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-82.862752 72.7\n",
      "-176.6580556 178.4419\n"
     ]
    }
   ],
   "source": [
    "print(df['latitude'].min(), df['latitude'].max())     # Should be roughly -90 to 90\n",
    "print(df['longitude'].min(), df['longitude'].max())   # Should be roughly -180 to 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd57960",
   "metadata": {},
   "source": [
    "Thankfully, all of our coordinates fall with typical ranges, so there's no further action required here. \n",
    "\n",
    "It was considered at this stage to utilise ChatGPT to create a 'helper function' in order to reverse-geocode the 'unknown' countries in the *country* column of our dataset. Upon further investigation, it was understood that this process would take the GeoPy API many hours to complete, and not without significant risk of potential setbacks.\n",
    "\n",
    "Approximately 12% of the rows in our UFO dataset were missing a recorded country. While it is technically possible to infer country from latitude and longitude via reverse geocoding, this was not implemented at scale due to performance and ethical limitations around API usage.\n",
    "\n",
    "Because this project is primarily concerned with **yearly patterns on a global scale**, the absence of country-level information does not significantly impact the core analysis or hypotheses being tested.\n",
    "\n",
    "This could be considered in a later iteration of our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f51c34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4007eb",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this section we aim to add new features the dataset in order to help us with our analysis.\n",
    "As our analysis goals are primarily concerned with **year-based** analysis, correlation and regression, we must add features that would help support our objectives.\n",
    "\n",
    "We consider our first stage to require adding a *year* column to our dataset, as this will serve as our primary anchor point. \n",
    "Secondly, we will create a column that gives us sightings per year. This will act as our primary dependent variable for regression analysis.\n",
    "\n",
    "We can also consider a count of sightings per country per year, allowing for increased granularity on our Dashboard, later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22004",
   "metadata": {},
   "source": [
    "Firstly, let's go ahead and create a column extracting the *year* from our *datetime* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb81c460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>2004-04-27</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "      <td>1949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>2005-12-16</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "      <td>1949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>2008-01-21</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "      <td>1955.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>2004-01-17</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "      <td>1956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>2004-01-22</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "      <td>1960.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  city    state  country     shape  \\\n",
       "0 1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1 1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2 1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3 1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4 1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "\n",
       "   duration_(seconds)                                           comments  \\\n",
       "0              2700.0  This event took place in early fall around 194...   \n",
       "1              7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                20.0  My older brother and twin sister were leaving ...   \n",
       "4               900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "\n",
       "  date_posted   latitude   longitude    year  \n",
       "0  2004-04-27  29.883056  -97.941111  1949.0  \n",
       "1  2005-12-16  29.384210  -98.581082  1949.0  \n",
       "2  2008-01-21  53.200000   -2.916667  1955.0  \n",
       "3  2004-01-17  28.978333  -96.645833  1956.0  \n",
       "4  2004-01-22  21.418056 -157.803611  1960.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column for year\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df.head() # Display the first five rows of the dataframe to check the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e3a516d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "year                         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efc8a2",
   "metadata": {},
   "source": [
    "We can see that our *year* column has been added to our dataset, but the data type clearly displays as a floating point number. We will need to change this to an integer. Before we do this, let us quickly check to see if there are any nulls in our column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a74435c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].isnull().sum() # Check for null values in the 'year' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da33a0",
   "metadata": {},
   "source": [
    "We can see that we have 694 null values in our *year* column (representing ~0.86% of our total data), so let us go ahead and drop these rows. \n",
    "Our decision for this is underpinned by the reason that we are primarily concerned with **year-based** analysis, and focusses on **trends over time**. We also require our *year* field to be valid for the purposes of merging with our *global_stress_events* dataset. \n",
    "\n",
    "Let us proceed then, to remove these unwanted rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abf63cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null values in the 'year' column\n",
    "df = df.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffeb5d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].isnull().sum()  # Check again for null values in the 'year' column after dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe1fa3",
   "metadata": {},
   "source": [
    "We can now see that we no longer have any NaN values in our *year* column. \n",
    "Let us proceed to convert our *year* data type from *Float64* to *Int*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4ac774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year to integer type\n",
    "df['year'] = df['year'].astype(int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d70eaf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "year                           int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the 'year' column is now of integer type\n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbab8fd",
   "metadata": {},
   "source": [
    "We have now successfully transformed our *year* column from 'float64' to 'int32' data type. \n",
    "\n",
    "Let us now move on to creating a summary DataFrame for our *sightings_per_year* requirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new summary DataFrame for sightings per year (using ChatGPT's suggestion)\n",
    "sightings_per_year = (\n",
    "    df.groupby('year')\n",
    "    .size()\n",
    "    .reset_index(name='sightings_per_year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf399e4e",
   "metadata": {},
   "source": [
    "Let us quickly check that this procedure has been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9db73ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sightings_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  sightings_per_year\n",
       "0  1906                   1\n",
       "1  1910                   1\n",
       "2  1916                   1\n",
       "3  1920                   1\n",
       "4  1925                   1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows to check the new DataFrame\n",
    "sightings_per_year.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ccb1e",
   "metadata": {},
   "source": [
    "Now that we have extracted our *sightings_per_year* to a new DataFrame, let us move on to handling our global_stress_events dataset. \n",
    "We notice that the years at the head of *sightings_per_year* fall outside of the date range of the global_stress_events data.\n",
    "In order to focus our data, it would seem prudent to align our years across datasets in order to only keep the years relevant to our analysis.\n",
    "\n",
    "Before we move on though, let us export our cleaned data set to our data/clean directory:\n",
    "\n",
    "This version includes:\n",
    "- Cleaned and standardised columns\n",
    "- Dropped invalid datetime entries\n",
    "- Cleaned and typecast numeric columns\n",
    "- Extracted *year* column for time-based aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc74ffeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>2004-04-27</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>2005-12-16</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>2008-01-21</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>2004-01-17</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>2004-01-22</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  city    state  country     shape  \\\n",
       "0 1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1 1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2 1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3 1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4 1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "\n",
       "   duration_(seconds)                                           comments  \\\n",
       "0              2700.0  This event took place in early fall around 194...   \n",
       "1              7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                20.0  My older brother and twin sister were leaving ...   \n",
       "4               900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "\n",
       "  date_posted   latitude   longitude  year  \n",
       "0  2004-04-27  29.883056  -97.941111  1949  \n",
       "1  2005-12-16  29.384210  -98.581082  1949  \n",
       "2  2008-01-21  53.200000   -2.916667  1955  \n",
       "3  2004-01-17  28.978333  -96.645833  1956  \n",
       "4  2004-01-22  21.418056 -157.803611  1960  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # Display the first few rows of the cleaned dataframe to check all changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849cc5be",
   "metadata": {},
   "source": [
    "Let us now filter our dates in order to marry up our two datasets correctly. Our global_stress_events dataset starts at the year 1947, and ends in 2023, but our primary UFO sightings data set starts and finishes earlier (1901-2013). Let's first filter our UFO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df54060b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>2004-04-27</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>2005-12-16</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>2008-01-21</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>2004-01-17</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>2004-01-22</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80327</th>\n",
       "      <td>2013-09-09 21:15:00</td>\n",
       "      <td>nashville</td>\n",
       "      <td>tn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>600.0</td>\n",
       "      <td>Round from the distance/slowly changing colors...</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>36.165833</td>\n",
       "      <td>-86.784444</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80328</th>\n",
       "      <td>2013-09-09 22:00:00</td>\n",
       "      <td>boise</td>\n",
       "      <td>id</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>Boise&amp;#44 ID&amp;#44 spherical&amp;#44 20 min&amp;#44 10 r...</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>43.613611</td>\n",
       "      <td>-116.202500</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80329</th>\n",
       "      <td>2013-09-09 22:00:00</td>\n",
       "      <td>napa</td>\n",
       "      <td>ca</td>\n",
       "      <td>us</td>\n",
       "      <td>other</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>Napa UFO&amp;#44</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>38.297222</td>\n",
       "      <td>-122.284444</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80330</th>\n",
       "      <td>2013-09-09 22:20:00</td>\n",
       "      <td>vienna</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Saw a five gold lit cicular craft moving fastl...</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>38.901111</td>\n",
       "      <td>-77.265556</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80331</th>\n",
       "      <td>2013-09-09 23:00:00</td>\n",
       "      <td>edmond</td>\n",
       "      <td>ok</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>2 witnesses 2  miles apart&amp;#44 Red &amp;amp; White...</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>35.652778</td>\n",
       "      <td>-97.477778</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77309 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime                  city    state  country     shape  \\\n",
       "0     1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1     1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2     1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3     1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4     1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "...                   ...                   ...      ...      ...       ...   \n",
       "80327 2013-09-09 21:15:00             nashville       tn       us     light   \n",
       "80328 2013-09-09 22:00:00                 boise       id       us    circle   \n",
       "80329 2013-09-09 22:00:00                  napa       ca       us     other   \n",
       "80330 2013-09-09 22:20:00                vienna       va       us    circle   \n",
       "80331 2013-09-09 23:00:00                edmond       ok       us     cigar   \n",
       "\n",
       "       duration_(seconds)                                           comments  \\\n",
       "0                  2700.0  This event took place in early fall around 194...   \n",
       "1                  7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                    20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                    20.0  My older brother and twin sister were leaving ...   \n",
       "4                   900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "...                   ...                                                ...   \n",
       "80327               600.0  Round from the distance/slowly changing colors...   \n",
       "80328              1200.0  Boise&#44 ID&#44 spherical&#44 20 min&#44 10 r...   \n",
       "80329              1200.0                                       Napa UFO&#44   \n",
       "80330                 5.0  Saw a five gold lit cicular craft moving fastl...   \n",
       "80331              1020.0  2 witnesses 2  miles apart&#44 Red &amp; White...   \n",
       "\n",
       "      date_posted   latitude   longitude  year  \n",
       "0      2004-04-27  29.883056  -97.941111  1949  \n",
       "1      2005-12-16  29.384210  -98.581082  1949  \n",
       "2      2008-01-21  53.200000   -2.916667  1955  \n",
       "3      2004-01-17  28.978333  -96.645833  1956  \n",
       "4      2004-01-22  21.418056 -157.803611  1960  \n",
       "...           ...        ...         ...   ...  \n",
       "80327  2013-09-30  36.165833  -86.784444  2013  \n",
       "80328  2013-09-30  43.613611 -116.202500  2013  \n",
       "80329  2013-09-30  38.297222 -122.284444  2013  \n",
       "80330  2013-09-30  38.901111  -77.265556  2013  \n",
       "80331  2013-09-30  35.652778  -97.477778  2013  \n",
       "\n",
       "[77309 rows x 11 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter dates to marry up with the global_stress_events dataset\n",
    "df = df[(df['year'] >= 1947) & (df['year'] <= 2013)]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3c1f2",
   "metadata": {},
   "source": [
    "We can now see that our date range now aligns with the years in our global_stress_events data.\n",
    "Let us now regenerate the sightings_per_year DataFrame to exclude the periods outside of the scope of our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6b1646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new summary DataFrame for sightings per year after filtering\n",
    "# using the same method as before\n",
    "sightings_per_year = (\n",
    "    df.groupby('year')\n",
    "    .size()\n",
    "    .reset_index(name='sightings_per_year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19cdf087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sightings_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1948</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2009</td>\n",
       "      <td>4508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2010</td>\n",
       "      <td>4258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2011</td>\n",
       "      <td>5076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012</td>\n",
       "      <td>7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2013</td>\n",
       "      <td>6988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  sightings_per_year\n",
       "0   1947                  35\n",
       "1   1948                   7\n",
       "2   1949                  16\n",
       "3   1950                  27\n",
       "4   1951                  20\n",
       "..   ...                 ...\n",
       "62  2009                4508\n",
       "63  2010                4258\n",
       "64  2011                5076\n",
       "65  2012                7308\n",
       "66  2013                6988\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sightings_per_year  # Display the first few rows to check the new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393ba26",
   "metadata": {},
   "source": [
    "We can see now that our sightings_per_year dates now align with our defined scope. \n",
    "Next we need to process and handle the global_stress_events.csv dataset. \n",
    "For this stage we will import the dataset using the *.read_csv()* method, format our column names/remove whitespace as we have done previously. We will also ensure that our *year* column is cast as *Int* type, and perform the same for our *severity_(1-5)* severity score column. \n",
    "We also wish to prepare the dataset for merging, where each year will have:\n",
    "- a count of how many stress events occurred\n",
    "- a sum of the severity scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87d1f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_df = pd.read_csv(\"../data/raw/global_stress_events.csv\") # Load the global stress events dataset\n",
    "stress_df.columns = stress_df.columns.str.strip().str.lower().str.replace(\" \", \"_\") # format column names\n",
    "stress_df['year'] = stress_df['year'].astype(int) # convert 'year' column to integer type\n",
    "stress_df['severity_(1-5)'] = pd.to_numeric(stress_df['severity_(1-5)'], errors='coerce') # convert 'severity_(1-5)' to numeric\n",
    "\n",
    "# Summarize stress events by year\n",
    "# This will create a DataFrame with the count of stress events and the sum of severity scores\n",
    "stress_summary = (\n",
    "    stress_df.groupby('year')\n",
    "    .agg(\n",
    "        stress_event_count=('severity_(1-5)', 'count'),\n",
    "        severity_sum=('severity_(1-5)', 'sum')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter to 1947–2013\n",
    "stress_summary = stress_summary[\n",
    "    (stress_summary['year'] >= 1947) & (stress_summary['year'] <= 2013)\n",
    "]\n",
    "# Code provided by ChatGPT for summarizing stress events by year, counting occurrences and summing severity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b89b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>event</th>\n",
       "      <th>category</th>\n",
       "      <th>region</th>\n",
       "      <th>severity_(1-5)</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>Roswell Incident</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>Key event in UFO mythology; public fascination...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1950</td>\n",
       "      <td>Korean War Begins</td>\n",
       "      <td>War</td>\n",
       "      <td>Asia/Global</td>\n",
       "      <td>4</td>\n",
       "      <td>Cold War era conflict that escalated nuclear f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1957</td>\n",
       "      <td>Launch of Sputnik</td>\n",
       "      <td>Tech/Political</td>\n",
       "      <td>Global</td>\n",
       "      <td>3</td>\n",
       "      <td>Cold War space race tensions rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1962</td>\n",
       "      <td>Cuban Missile Crisis</td>\n",
       "      <td>Political</td>\n",
       "      <td>Global</td>\n",
       "      <td>5</td>\n",
       "      <td>Closest point to nuclear war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1965</td>\n",
       "      <td>Vietnam War Escalates</td>\n",
       "      <td>War</td>\n",
       "      <td>Asia/US</td>\n",
       "      <td>4</td>\n",
       "      <td>Mass Protest, global media coverage, deep publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1969</td>\n",
       "      <td>Moon Landing</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>3</td>\n",
       "      <td>Heightened space fascination and speculation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1973</td>\n",
       "      <td>Oil Crisis Begins</td>\n",
       "      <td>Economic</td>\n",
       "      <td>Global</td>\n",
       "      <td>4</td>\n",
       "      <td>Fuel shortages, inflation, unrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1977</td>\n",
       "      <td>Release of Close Encounters</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>1</td>\n",
       "      <td>Popularised UFOs in mainstream media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1982</td>\n",
       "      <td>E.T. the Extra-Terrestrial Released</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>2</td>\n",
       "      <td>Global box office hit; shifted alien narrative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1986</td>\n",
       "      <td>Chernobyl Disaster</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>Europe</td>\n",
       "      <td>4</td>\n",
       "      <td>Massive nuclear fear and cover-up concerns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1989</td>\n",
       "      <td>Fall of Berlin Wall</td>\n",
       "      <td>Political</td>\n",
       "      <td>Europe</td>\n",
       "      <td>3</td>\n",
       "      <td>Geopolitical restructuring, global tension shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1991</td>\n",
       "      <td>Gulf War Begins</td>\n",
       "      <td>War</td>\n",
       "      <td>Middle East/US</td>\n",
       "      <td>4</td>\n",
       "      <td>Heavy media coverage, aerial warfare focus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1993</td>\n",
       "      <td>Waco Siege</td>\n",
       "      <td>Political</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>Domestic unrest, government distrust spikes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1993</td>\n",
       "      <td>X-Files TV Show Premieres</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>2</td>\n",
       "      <td>Long-running cultural influence on UFO and gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1995</td>\n",
       "      <td>Oklahoma City Bombing</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>Domestic terrorism, fear of extremism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1996</td>\n",
       "      <td>Independence Day Film</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>1</td>\n",
       "      <td>Blockbuster alien invasion film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1997</td>\n",
       "      <td>Heaven's Gate Suicides</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>UFO cult mass suicide linked to Hale-Bopp comet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1999</td>\n",
       "      <td>Y2K Panic</td>\n",
       "      <td>Tech</td>\n",
       "      <td>Global</td>\n",
       "      <td>3</td>\n",
       "      <td>Widespread fear of tech collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>2001</td>\n",
       "      <td>9/11 Terror Attacks</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>Global</td>\n",
       "      <td>5</td>\n",
       "      <td>Massive global psychological impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>2003</td>\n",
       "      <td>Iraq War Begins</td>\n",
       "      <td>War</td>\n",
       "      <td>Global</td>\n",
       "      <td>4</td>\n",
       "      <td>Anti-war protests, fear of global instability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2004</td>\n",
       "      <td>Indian Ocean Tsunami</td>\n",
       "      <td>Disaster</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4</td>\n",
       "      <td>Massive loss of life, global humanitarian resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2008</td>\n",
       "      <td>Global Financial Crisis</td>\n",
       "      <td>Economic</td>\n",
       "      <td>Global</td>\n",
       "      <td>5</td>\n",
       "      <td>Widespread unemployment, housing collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2010</td>\n",
       "      <td>Haiti Earthquake</td>\n",
       "      <td>Disaster</td>\n",
       "      <td>Americas</td>\n",
       "      <td>3</td>\n",
       "      <td>Media-heavy coverage, humanitarian crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>2011</td>\n",
       "      <td>Arab Spring Begins</td>\n",
       "      <td>Political</td>\n",
       "      <td>Middle East</td>\n",
       "      <td>4</td>\n",
       "      <td>Uprisings and major instability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2011</td>\n",
       "      <td>Fukushima Disaster</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4</td>\n",
       "      <td>Nuclear crisis and fear of radiation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2012</td>\n",
       "      <td>Mayan Apocalypse Prediction</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>2</td>\n",
       "      <td>Public anxiety over end-of-world prophecy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>2014</td>\n",
       "      <td>Crimea Annexation</td>\n",
       "      <td>Political</td>\n",
       "      <td>Europe</td>\n",
       "      <td>3</td>\n",
       "      <td>Tensions between Russia and West increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>2016</td>\n",
       "      <td>Brexit Referendum</td>\n",
       "      <td>Political</td>\n",
       "      <td>UK</td>\n",
       "      <td>3</td>\n",
       "      <td>National uncertainty and deep division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2016</td>\n",
       "      <td>Trump Elected President</td>\n",
       "      <td>Political</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>Global reaction and polarisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>2017</td>\n",
       "      <td>North Korea Missile Tensions</td>\n",
       "      <td>Military</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4</td>\n",
       "      <td>Nuclear fears escalated by media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>2019</td>\n",
       "      <td>Notre-Dame Fire</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>Europe</td>\n",
       "      <td>2</td>\n",
       "      <td>Global shock and symbolic destruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>2020</td>\n",
       "      <td>COVID-19 Pandemic Begins</td>\n",
       "      <td>Health</td>\n",
       "      <td>Global</td>\n",
       "      <td>5</td>\n",
       "      <td>Lockdowns, mass fear, health crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>2020</td>\n",
       "      <td>Australian Bushfire Crisis</td>\n",
       "      <td>Environmental</td>\n",
       "      <td>Australia/Global</td>\n",
       "      <td>4</td>\n",
       "      <td>One of the worst fire seasons; mass displaceme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>2022</td>\n",
       "      <td>Russia Invades Ukraine</td>\n",
       "      <td>War</td>\n",
       "      <td>Global</td>\n",
       "      <td>5</td>\n",
       "      <td>War, inflation, nuclear fears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>2023</td>\n",
       "      <td>ChatGPT Public Release</td>\n",
       "      <td>Tech/Cultural</td>\n",
       "      <td>Global</td>\n",
       "      <td>2</td>\n",
       "      <td>Public discourse on AI risk and ethics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  year                                event        category  \\\n",
       "0    1  1947                     Roswell Incident        Cultural   \n",
       "1    2  1950                    Korean War Begins             War   \n",
       "2    3  1957                    Launch of Sputnik  Tech/Political   \n",
       "3    4  1962                 Cuban Missile Crisis       Political   \n",
       "4    5  1965                Vietnam War Escalates             War   \n",
       "5    6  1969                         Moon Landing        Cultural   \n",
       "6    7  1973                    Oil Crisis Begins        Economic   \n",
       "7    8  1977          Release of Close Encounters        Cultural   \n",
       "8    9  1982  E.T. the Extra-Terrestrial Released        Cultural   \n",
       "9   10  1986                   Chernobyl Disaster   Environmental   \n",
       "10  11  1989                  Fall of Berlin Wall       Political   \n",
       "11  12  1991                      Gulf War Begins             War   \n",
       "12  13  1993                           Waco Siege       Political   \n",
       "13  14  1993            X-Files TV Show Premieres        Cultural   \n",
       "14  15  1995                Oklahoma City Bombing       Terrorism   \n",
       "15  16  1996                Independence Day Film        Cultural   \n",
       "16  17  1997               Heaven's Gate Suicides        Cultural   \n",
       "17  18  1999                            Y2K Panic            Tech   \n",
       "18  19  2001                  9/11 Terror Attacks       Terrorism   \n",
       "19  20  2003                      Iraq War Begins             War   \n",
       "20  21  2004                 Indian Ocean Tsunami        Disaster   \n",
       "21  22  2008              Global Financial Crisis        Economic   \n",
       "22  23  2010                     Haiti Earthquake        Disaster   \n",
       "23  24  2011                   Arab Spring Begins       Political   \n",
       "24  25  2011                   Fukushima Disaster   Environmental   \n",
       "25  26  2012          Mayan Apocalypse Prediction        Cultural   \n",
       "26  27  2014                    Crimea Annexation       Political   \n",
       "27  28  2016                    Brexit Referendum       Political   \n",
       "28  29  2016              Trump Elected President       Political   \n",
       "29  30  2017         North Korea Missile Tensions        Military   \n",
       "30  31  2019                      Notre-Dame Fire        Cultural   \n",
       "31  32  2020             COVID-19 Pandemic Begins          Health   \n",
       "32  33  2020           Australian Bushfire Crisis   Environmental   \n",
       "33  34  2022               Russia Invades Ukraine             War   \n",
       "34  35  2023               ChatGPT Public Release   Tech/Cultural   \n",
       "\n",
       "              region  severity_(1-5)  \\\n",
       "0                 US               2   \n",
       "1        Asia/Global               4   \n",
       "2             Global               3   \n",
       "3             Global               5   \n",
       "4            Asia/US               4   \n",
       "5             Global               3   \n",
       "6             Global               4   \n",
       "7             Global               1   \n",
       "8             Global               2   \n",
       "9             Europe               4   \n",
       "10            Europe               3   \n",
       "11    Middle East/US               4   \n",
       "12                US               2   \n",
       "13            Global               2   \n",
       "14                US               3   \n",
       "15            Global               1   \n",
       "16                US               2   \n",
       "17            Global               3   \n",
       "18            Global               5   \n",
       "19            Global               4   \n",
       "20              Asia               4   \n",
       "21            Global               5   \n",
       "22          Americas               3   \n",
       "23       Middle East               4   \n",
       "24              Asia               4   \n",
       "25            Global               2   \n",
       "26            Europe               3   \n",
       "27                UK               3   \n",
       "28                US               3   \n",
       "29              Asia               4   \n",
       "30            Europe               2   \n",
       "31            Global               5   \n",
       "32  Australia/Global               4   \n",
       "33            Global               5   \n",
       "34            Global               2   \n",
       "\n",
       "                                                notes  \n",
       "0   Key event in UFO mythology; public fascination...  \n",
       "1   Cold War era conflict that escalated nuclear f...  \n",
       "2                   Cold War space race tensions rise  \n",
       "3                        Closest point to nuclear war  \n",
       "4   Mass Protest, global media coverage, deep publ...  \n",
       "5        Heightened space fascination and speculation  \n",
       "6                   Fuel shortages, inflation, unrest  \n",
       "7                Popularised UFOs in mainstream media  \n",
       "8   Global box office hit; shifted alien narrative...  \n",
       "9          Massive nuclear fear and cover-up concerns  \n",
       "10   Geopolitical restructuring, global tension shift  \n",
       "11         Heavy media coverage, aerial warfare focus  \n",
       "12        Domestic unrest, government distrust spikes  \n",
       "13  Long-running cultural influence on UFO and gov...  \n",
       "14              Domestic terrorism, fear of extremism  \n",
       "15                    Blockbuster alien invasion film  \n",
       "16    UFO cult mass suicide linked to Hale-Bopp comet  \n",
       "17                   Widespread fear of tech collapse  \n",
       "18                Massive global psychological impact  \n",
       "19      Anti-war protests, fear of global instability  \n",
       "20  Massive loss of life, global humanitarian resp...  \n",
       "21          Widespread unemployment, housing collapse  \n",
       "22          Media-heavy coverage, humanitarian crisis  \n",
       "23                    Uprisings and major instability  \n",
       "24               Nuclear crisis and fear of radiation  \n",
       "25          Public anxiety over end-of-world prophecy  \n",
       "26          Tensions between Russia and West increase  \n",
       "27             National uncertainty and deep division  \n",
       "28                   Global reaction and polarisation  \n",
       "29                   Nuclear fears escalated by media  \n",
       "30              Global shock and symbolic destruction  \n",
       "31                Lockdowns, mass fear, health crisis  \n",
       "32  One of the worst fire seasons; mass displaceme...  \n",
       "33                      War, inflation, nuclear fears  \n",
       "34             Public discourse on AI risk and ethics  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the global stress events DataFrame\n",
    "stress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9cd76bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>stress_event_count</th>\n",
       "      <th>severity_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1957</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1965</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1973</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1982</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1989</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1993</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  stress_event_count  severity_sum\n",
       "0   1947                   1             2\n",
       "1   1950                   1             4\n",
       "2   1957                   1             3\n",
       "3   1962                   1             5\n",
       "4   1965                   1             4\n",
       "5   1969                   1             3\n",
       "6   1973                   1             4\n",
       "7   1977                   1             1\n",
       "8   1982                   1             2\n",
       "9   1986                   1             4\n",
       "10  1989                   1             3\n",
       "11  1991                   1             4\n",
       "12  1993                   2             4\n",
       "13  1995                   1             3\n",
       "14  1996                   1             1\n",
       "15  1997                   1             2\n",
       "16  1999                   1             3\n",
       "17  2001                   1             5\n",
       "18  2003                   1             4\n",
       "19  2004                   1             4\n",
       "20  2008                   1             5\n",
       "21  2010                   1             3\n",
       "22  2011                   2             8\n",
       "23  2012                   1             2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39907fa4",
   "metadata": {},
   "source": [
    "We can see from our checks here that all of our processes have worked, our dates align, our severity and stress sums are correct, and so we are now clear to export our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ac90d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned and time-aligned UFO sightings data\n",
    "df.to_csv(\"../data/clean/ufo_sightings_cleaned_aligned.csv\", index=False)\n",
    "\n",
    "# Export yearly UFO sightings summary\n",
    "sightings_per_year.to_csv(\"../data/clean/sightings_per_year.csv\", index=False)\n",
    "\n",
    "# Export cleaned and summarised global stress events\n",
    "stress_summary.to_csv(\"../data/clean/global_stress_events_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0cc80",
   "metadata": {},
   "source": [
    "### Export Summary\n",
    "\n",
    "Three key datasets were exported following the data cleaning and alignment process:\n",
    "\n",
    "- 'ufo_sightings_cleaned_aligned.csv': Full UFO dataset filtered to years 1947–2013\n",
    "- 'sightings_per_year.csv': Aggregated UFO sightings per year\n",
    "- 'global_stress_events_summary.csv': Aggregated stress event count and severity per year\n",
    "\n",
    "These datasets are now ready for merging and statistical analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ec014",
   "metadata": {},
   "source": [
    "Now we are clear to merge our *sightings_per_year* and *global_stress_events_summary* datasets together. \n",
    "We wish to consider which type of join we wish to use. We would like to avoid biasing our data by keeping all years that have sightings, and not just the years which align with the global_stress_events data - as this would exclude all sightings that occurred in non-stress years. \n",
    "Because of this, we use *year* as our merging point, and use the *left* join to keep all years where sightings occurred. \n",
    "This will likely cause non-stress years to be populated with *NaN* values, which we will then need to handle after joining. Our non-stress/low-stress years will form a vital component for our regression analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0aa10102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the UFO sightings per year with the stress summary DataFrame\n",
    "merged_ufo_df = pd.merge(\n",
    "    sightings_per_year,\n",
    "    stress_summary,\n",
    "    on='year',\n",
    "    how='left'  # Keep all years where sightings occurred\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40e9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sightings_per_year</th>\n",
       "      <th>stress_event_count</th>\n",
       "      <th>severity_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1948</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2009</td>\n",
       "      <td>4508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2010</td>\n",
       "      <td>4258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2011</td>\n",
       "      <td>5076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012</td>\n",
       "      <td>7308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2013</td>\n",
       "      <td>6988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  sightings_per_year  stress_event_count  severity_sum\n",
       "0   1947                  35                 1.0           2.0\n",
       "1   1948                   7                 NaN           NaN\n",
       "2   1949                  16                 NaN           NaN\n",
       "3   1950                  27                 1.0           4.0\n",
       "4   1951                  20                 NaN           NaN\n",
       "..   ...                 ...                 ...           ...\n",
       "62  2009                4508                 NaN           NaN\n",
       "63  2010                4258                 1.0           3.0\n",
       "64  2011                5076                 2.0           8.0\n",
       "65  2012                7308                 1.0           2.0\n",
       "66  2013                6988                 NaN           NaN\n",
       "\n",
       "[67 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_ufo_df # check the merged DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434ae12",
   "metadata": {},
   "source": [
    "Now that we have merged our two dataframes, we can see that we have the expected NaN values. Let us handle this in a similar way to how we did previously. The main difference here is we will not populate with 'unknown' as before, but keep the scores numeric - so we will replace the NaN with Zeroes (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9996bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in stress_event_count and severity_sum with 0\n",
    "merged_ufo_df['stress_event_count'] = merged_ufo_df['stress_event_count'].fillna(0).astype(int)\n",
    "merged_ufo_df['severity_sum'] = merged_ufo_df['severity_sum'].fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e14b8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sightings_per_year</th>\n",
       "      <th>stress_event_count</th>\n",
       "      <th>severity_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1948</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2009</td>\n",
       "      <td>4508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2010</td>\n",
       "      <td>4258</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2011</td>\n",
       "      <td>5076</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012</td>\n",
       "      <td>7308</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2013</td>\n",
       "      <td>6988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  sightings_per_year  stress_event_count  severity_sum\n",
       "0   1947                  35                   1             2\n",
       "1   1948                   7                   0             0\n",
       "2   1949                  16                   0             0\n",
       "3   1950                  27                   1             4\n",
       "4   1951                  20                   0             0\n",
       "..   ...                 ...                 ...           ...\n",
       "62  2009                4508                   0             0\n",
       "63  2010                4258                   1             3\n",
       "64  2011                5076                   2             8\n",
       "65  2012                7308                   1             2\n",
       "66  2013                6988                   0             0\n",
       "\n",
       "[67 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_ufo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c5e5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the merged DataFrame to a CSV file\n",
    "merged_ufo_df.to_csv(\"../data/clean/merged_ufo_stress_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fc217",
   "metadata": {},
   "source": [
    "### Final Merged Dataset Export Summary\n",
    "The cleaned and aligned **'sightings_per_year'** dataset was merged with the **'global_stress events_summary'** dataset using a *left join*.\n",
    "This preserves all years with recorded UFO sightings (1947-2013), while filling in 0 values for years without recorded global stress events.\n",
    "The resulting dataset, **'merged_ufo_stress_data.csv'** contains:\n",
    "- year\n",
    "- sightings_per_year count\n",
    "- stress_event_count\n",
    "- severity_sum which gives each year a total global stress severity score. \n",
    "\n",
    "Saved as: '..data/clean/merged_ufo_stress_data.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10e145",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering Summary\n",
    "\n",
    "The UFO dataset underwent comprehensive cleaning to ensure analytical reliability. Key steps included:\n",
    "\n",
    "- **Standardising column names** to lowercase with consistent formatting.\n",
    "- **Handling missing values:**\n",
    "  - Filled missing 'country', 'state', and 'shape' values with 'unknown'.\n",
    "  - Removed 694 rows (~0.86%) where 'datetime' was missing or malformed, resulting in invalid 'year' values.\n",
    "- **Cleaning problematic numeric columns:**\n",
    "  - Converted 'duration (seconds)' and 'latitude' to numeric using 'pd.to_numeric()' with coercion.\n",
    "  - Dropped 4 rows with invalid or non-numeric duration/latitude values.\n",
    "- **Ensuring valid data ranges:**\n",
    "  - Confirmed all latitude/longitude values were within Earth’s bounds.\n",
    "  - Verified 'year' column ranged between 1947 and 2013 to align with the global stress dataset.\n",
    "- **Dropped duplicate rows:** 2 exact duplicate entries were removed.\n",
    "- **Converted data types:**\n",
    "  - 'datetime' and 'date_posted' columns converted to proper datetime format.\n",
    "  - Categorical fields ('country', 'state', 'shape') cast to 'category' type for efficient storage.\n",
    "  - 'year' column cast to 'int' after NaNs were removed.\n",
    "\n",
    "### Feature Engineering Summary\n",
    "\n",
    "To prepare for analysis and regression, the following features were created:\n",
    "\n",
    "- **'year'** extracted from the 'datetime' column to serve as a temporal anchor for all analysis.\n",
    "- **'sightings_per_year'**: A new summary DataFrame was created by grouping the cleaned UFO dataset by 'year', counting the number of sightings annually.\n",
    "- The global stress dataset was also cleaned:\n",
    "  - Columns standardised and typed correctly.\n",
    "  - Aggregated into a yearly summary with:\n",
    "    - 'stress_event_count' (number of stress events per year)\n",
    "    - 'severity_sum' (total of severity scores per year).\n",
    "- Both datasets were filtered to a shared timeframe (1947–2013) and merged using a **left join**, preserving years with zero stress events for baseline comparison.\n",
    "- Missing values in the merged dataset (due to non-stress years) were filled with '0' to maintain continuity in analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb341cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
