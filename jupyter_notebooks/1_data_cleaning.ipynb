{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6340765",
   "metadata": {},
   "source": [
    "# **The Stress-Sighting Hypothesis**\n",
    "## A Data-Driven Analysis of Global Events and Reports of the Unknown.\n",
    "\n",
    "**The Stress-Sighting Hypothesis** has the project goal of investigating whether there is a meaningful correlation between the frequency of reported UFO sightings and periods of heightened cultural, political or global stress, using historical event data and publicly reported sightings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54068032",
   "metadata": {},
   "source": [
    "## User Story\n",
    "Alex Holloway is an investigative journalist, known for in-depth features that combine cultural analysis with data storytelling. They work with both independent media outlets and major publishers, seeking to explore how society processes uncertainty — from political unrest to media myths.\n",
    "\n",
    "Alex is planning to write an article on how Global Stress Events impact the number of UFO sightings, and has asked us to conduct our analysis, using the publicly available NUFORC (National UFO Reporting Centre) UFO Sightings dataset found [here](https://www.kaggle.com/datasets/NUFORC/ufo-sightings/data)\n",
    "\n",
    "## Business Requirements\n",
    "In an era shaped by information saturation, political polarisation, and global crises, public perception is increasingly complex and emotionally charged. For journalists, researchers, and communicators, understanding how people respond to uncertainty is as important as the events themselves.\n",
    "\n",
    "This project explores the potential relationship between **reported UFO sightings** and **global stress events**, not to investigate extraterrestrial phenomena, but to examine whether these sightings reflect **underlying patterns of public anxiety, media influence, and cultural tension.**\n",
    "\n",
    "The outcome is a data-driven dashboard designed to support those working at the intersection of **data**, **storytelling**, and **public insight**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bd970",
   "metadata": {},
   "source": [
    "![Alex Holloway – Persona Card](../images/alex_holloway_persona_card.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1b80e",
   "metadata": {},
   "source": [
    "### Alex's Requirements:\n",
    "\n",
    "- **Reveal Patterns**\n",
    "\n",
    "Alex needs to identify correlations between historical periods of stress and spikes in UFO reporting - fast, clearly and without technical issues. \n",
    "\n",
    "- **Narrative Context**\n",
    "\n",
    "They want to explore not just *when* things happened, but *why it matters.* Explanatory text and annotations support deeper storytelling.\n",
    "\n",
    "- **Usable Insights**\n",
    "\n",
    "Our charts and summaries must be easy to extract for use in articles or reports, including explanatory captions and legends.\n",
    "\n",
    "- **Trustworthy Structure**\n",
    "\n",
    "The data pipeline must be transparent, ethical and well-documented to ensure and maintain credibility in their journalistic work.\n",
    "\n",
    "### Value Proposition:\n",
    "Our Dashboard must empower users like Alex to:\n",
    "- Translate complex data into cultural insight\n",
    "- Frame journalistic stories with empirical evidence\n",
    "- Uncover social signals hiding in unconventional data\n",
    "- Offer the audience a grounded perspective on how fear, media, and uncertainty intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31b00e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d0ae1",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "Our Hypotheses for this project are as follows:\n",
    "\n",
    "### **Hypothesis 1:** \n",
    "\n",
    "**There is a positive correlation between the number of glabal stress events in a given year and the number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 2:**\n",
    "\n",
    "**Years with higher total stress severity scores are associated with a greater number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 3:**\n",
    "\n",
    "**Cultural media events, (such as the release of UFO-themed films or television series) correspond with noticeable short-term spikes in reported sightings.**\n",
    "\n",
    "For the sake of brevity, we will not outline our validation approaches here, as this will be covered in a seperate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd704a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d992c",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning\n",
    "\n",
    "In this section we will look to extract our data and give consideration to how we will clean it in order to make it effective for analysis. \n",
    "Our first step is to load our first dataset: *ufo_data_scrubbed.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283daf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hidde\\AppData\\Local\\Temp\\ipykernel_21784\\3637515765.py:7: DtypeWarning: Columns (5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries and load dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a70b6d",
   "metadata": {},
   "source": [
    "Straight away we get a data type warning advising us that columns 5 & 9 have mixed data types. This is less than ideal, and will cause issues further down the line when we attempt to merge, aggregate or model our data. \n",
    "\n",
    "let's go ahead and check our columns with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35bd4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime',\n",
       " 'city',\n",
       " 'state',\n",
       " 'country',\n",
       " 'shape',\n",
       " 'duration (seconds)',\n",
       " 'duration (hours/min)',\n",
       " 'comments',\n",
       " 'date posted',\n",
       " 'latitude',\n",
       " 'longitude ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list() # list all columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c5431",
   "metadata": {},
   "source": [
    "Based on the principle of zero-indexing, we can see that our 'duration (seconds)' and 'latitude' columns are likely to be our offenders here. \n",
    "I'll now consult with ChatGPT to suggest code to help identify the problems in our code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92024b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "['2`' '8`' '0.5`']\n",
      "Non-numeric values in latitude:\n",
      "['33q.200088']\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fce55",
   "metadata": {},
   "source": [
    "We can see from the code output that we have some uexpected, non-numeric characters populating several rows. \n",
    "Let's now convert these columns to strictly numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45867502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration (seconds)'] = pd.to_numeric(df['duration (seconds)'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "# coerce will convert non-numeric values to NaN\n",
    "# Code provided by ChatGPT to convert columns to numeric types, handling non-numeric values by converting them to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c65f5",
   "metadata": {},
   "source": [
    "Now, let's run our Helper Function again to check that things have been resolved as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef2deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "[nan]\n",
      "Non-numeric values in latitude:\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d2002",
   "metadata": {},
   "source": [
    "We can now see that we have replaced the non-numeric values with the NaN (Not a Number) value.\n",
    "Let us now flag the number or rows to be dropped, and export the dropped rows to a new .csv file for the purposes of auditing and transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39268af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag rows with invalid (non-numeric) duration or latitude\n",
    "df['invalid_duration_or_latitude'] = df[['duration (seconds)', 'latitude']].isnull().any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37846883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to be dropped: 4\n"
     ]
    }
   ],
   "source": [
    "# Count and optionally save them\n",
    "dropped_rows = df[df['invalid_duration_or_latitude']]\n",
    "print(f\"Number of rows to be dropped: {len(dropped_rows)}\")\n",
    "\n",
    "# Export dropped rows for audit\n",
    "dropped_rows.to_csv(\"../data/dropped_invalid_coordinates_or_duration.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf9d6f",
   "metadata": {},
   "source": [
    "As we can see, there are only 4 rows flagged to be dropped here, which represents ~0.005% of our total data, so let's go ahead and drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b88476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with invalid duration or latitude\n",
    "df = df[~df['invalid_duration_or_latitude']].drop(columns='invalid_duration_or_latitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479475d",
   "metadata": {},
   "source": [
    "Now let us check that our NaN values have been dropped from the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cf27cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (seconds)    0\n",
      "latitude              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['duration (seconds)', 'latitude']].isnull().sum())\n",
    "# check that there are no more NaN values in the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f0b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27822</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43782</th>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.440663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration (seconds)   latitude\n",
       "27822                 NaN  33.932500\n",
       "35692                 NaN  36.974167\n",
       "43782               180.0        NaN\n",
       "58591                 NaN   4.440663"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few of the previously dropped values\n",
    "dropped_rows[['duration (seconds)', 'latitude']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9042070",
   "metadata": {},
   "source": [
    "Here we can see that we have successfully removed the rows with NaN values, and that, as expected there are only 4 rows removed.\n",
    "These rows represent such a small fraction of the data (~0.005%) that their absence will not introduce bias, distort correlations, or meaningfully affect the outcome of any regression or visual insights. Removing them ensures a cleaner, more reliable dataset without sacrificing representativeness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77f849",
   "metadata": {},
   "source": [
    "Now that we have solved our initial issue of unexpected characters appearing in the *duration (seconds)* column and the *latitude* column, let us now continue by conducting a broad audit of missing data across the entire dataset. We can conduct a very simple operation here by using a combination of the 'isnull()' and 'sum()' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5b436d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                   5796\n",
       "country                 9668\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae66e6",
   "metadata": {},
   "source": [
    "We can see here that despite using the 'scrubbed' version of our UFO data, we still have a lot of missing values to deal with. \n",
    "Let us quickly calculate the percentage of the whole database that has the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3824d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                 12.04\n",
       "state                    7.22\n",
       "shape                    2.40\n",
       "comments                 0.02\n",
       "datetime                 0.00\n",
       "city                     0.00\n",
       "duration (seconds)       0.00\n",
       "duration (hours/min)     0.00\n",
       "date posted              0.00\n",
       "latitude                 0.00\n",
       "longitude                0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show percentage of missing values\n",
    "(df.isnull().sum() / len(df) * 100).round(2).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172be28",
   "metadata": {},
   "source": [
    "We can see from this that missing *country* values make up ~12% of our dataset. Missing *state* entries account for ~7.2%.\n",
    "Missing *shape* decriptors accoount for only 2.4%, and missing *comments* only 0.02%.\n",
    "\n",
    "In this instance, let us first turn our attention to resolving the missing *country* values. Due to the statistically significant proportion of our dataset that this represents, we decide to impute the missing values with \"Unknown\" rather than deleting the rows entirely. \n",
    "\n",
    "We can perform this operation using the following methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676c8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing country values with 'unknown'\n",
    "df['country'] = df['country'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597677a4",
   "metadata": {},
   "source": [
    "let us check that this has worked as anticipating by running our previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfda9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                   5796\n",
       "country                    0\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd574e",
   "metadata": {},
   "source": [
    "Great! We can see that our *country* column now has zero missing entries, so our imputation has been successful.\n",
    "We can now perform the same operation on the *state* columns. We've decided to take this course of action due to the dataset containing sightings that have occurred in regions outside of the US, and so may not require or have *state* values. These rows may still have relevance to our regional breakdown analysis that we may conduct later on. \n",
    "Let us perform the same operation as before, but alter our code to point to the *state* column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2da068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing state values with 'unknown'\n",
    "df['state'] = df['state'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761e722",
   "metadata": {},
   "source": [
    "Again, let us run our test to ensure that our operation has been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc788534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                   0\n",
       "city                       0\n",
       "state                      0\n",
       "country                    0\n",
       "shape                   1930\n",
       "duration (seconds)         0\n",
       "duration (hours/min)       0\n",
       "comments                  15\n",
       "date posted                0\n",
       "latitude                   0\n",
       "longitude                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde60f03",
   "metadata": {},
   "source": [
    "Success! Let us now move on to addressing the missing values in the *shape* column. Once again, it seems prudent for us to impute 'unknown' values into the missing values here; due to this column potentially feeding into later visulaisations factoring shape type as a notable interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f90d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing shape values with 'unknown'\n",
    "df['shape'] = df['shape'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d423ce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 0\n",
       "city                     0\n",
       "state                    0\n",
       "country                  0\n",
       "shape                    0\n",
       "duration (seconds)       0\n",
       "duration (hours/min)     0\n",
       "comments                15\n",
       "date posted              0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e12d8",
   "metadata": {},
   "source": [
    "We have now solved the majority of our missing values, with only the missing *comments* values remaining. We have two options for resolving this issue. Either we could fill this missing entries with empty string values, or drop the rows entirely. \n",
    "As we saw earlier, these missing values only account for 0.02% of our data, and so because of the low significance to our overall analysis, we decide to drop these rows. \n",
    "For this we use the *dropna()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cca095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing comments\n",
    "df = df.dropna(subset=['comments'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9d2351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                0\n",
       "city                    0\n",
       "state                   0\n",
       "country                 0\n",
       "shape                   0\n",
       "duration (seconds)      0\n",
       "duration (hours/min)    0\n",
       "comments                0\n",
       "date posted             0\n",
       "latitude                0\n",
       "longitude               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing value counts in the entire dataframe\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d13774",
   "metadata": {},
   "source": [
    "Now that we have successfully handled our missing data entries, let us summarise our handling decisions:\n",
    "\n",
    "- *country* : ~12% missing data filled with 'unknown'\n",
    "- *state* : ~7.2% missing data filled with 'unknown'\n",
    "- *shape* : ~2.4% missing data filled with 'unknown'\n",
    "- *comments* : ~0.02% missing data dropped. \n",
    "\n",
    "These decisions were made in order to preserve the maximum data integrity, while allowing us flexibility in filtering and consistent formatting in categorical fields for visual analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cac8ea",
   "metadata": {},
   "source": [
    "Next, let us quickly ensure that our column names are standardised, due to us seeing that some columns contain spaces, for example the *duration (seconds)* column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50317f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'city', 'state', 'country', 'shape', 'duration_(seconds)',\n",
       "       'duration_(hours/min)', 'comments', 'date_posted', 'latitude',\n",
       "       'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace spaces in column names with underscores and convert to lowercase\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222acb8",
   "metadata": {},
   "source": [
    "We can now see that our column names have been standardised, and that we have solved the issues with column names having spaces. This will help us later on when we come to merge our datasets, and also for when we start to conduct our analysis. \n",
    "\n",
    "Next, we should check that our columns are correctly assigned the proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b011d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                 object\n",
       "city                     object\n",
       "state                    object\n",
       "country                  object\n",
       "shape                    object\n",
       "duration_(seconds)      float64\n",
       "duration_(hours/min)     object\n",
       "comments                 object\n",
       "date_posted              object\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data types of the columns\n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24f5d5",
   "metadata": {},
   "source": [
    "We can see from our data types check that there are some columns that will need to have their data types changed. \n",
    "First on our 'to-do' list is handling the *datetime* column - from 'object' to 'datetime.' We will do this by utilising the *.to_datetime()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4be02b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime                datetime64[ns]\n",
       "city                            object\n",
       "state                           object\n",
       "country                         object\n",
       "shape                           object\n",
       "duration_(seconds)             float64\n",
       "duration_(hours/min)            object\n",
       "comments                        object\n",
       "date_posted                     object\n",
       "latitude                       float64\n",
       "longitude                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'datetime' column to datetime type\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "#  add the .dtypes function to check the data types again  \n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a9d71",
   "metadata": {},
   "source": [
    "Now that we have converted our *datetime* column to the correct format, let us move on to handling the next column with the incorrect data type - *duration_(hours/min)*\n",
    "\n",
    "Addressing the requirements of this column tells us that it is not needed for our analysis, due to it having inconsistent formatting throughout, so would require some serious, time-consuming parsing. As we also have a *duration_(seconds)* column, we feel that the *duration_(hours/mins)* column is redundant for our purposes. \n",
    "\n",
    "Let us proceed to drop this column from our dataset using the *.drop()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e20fe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'duration_(hours/min)' column as it is redundant\n",
    "df.drop(columns=['duration_(hours/min)'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e20d2",
   "metadata": {},
   "source": [
    "Now that we have removed the column, let us quickly check that it has been successfully removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55dfa424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  city    state  country     shape  \\\n",
       "0 1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1 1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2 1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3 1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4 1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "\n",
       "   duration_(seconds)                                           comments  \\\n",
       "0              2700.0  This event took place in early fall around 194...   \n",
       "1              7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                20.0  My older brother and twin sister were leaving ...   \n",
       "4               900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "\n",
       "  date_posted   latitude   longitude  \n",
       "0   4/27/2004  29.883056  -97.941111  \n",
       "1  12/16/2005  29.384210  -98.581082  \n",
       "2   1/21/2008  53.200000   -2.916667  \n",
       "3   1/17/2004  28.978333  -96.645833  \n",
       "4   1/22/2004  21.418056 -157.803611  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # Display the first few rows of the cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc4b48",
   "metadata": {},
   "source": [
    "We can see that our *duration_(hours/min) column has been successfully removed. \n",
    "Next, let us convert the *date_posted* column into from an 'object' to the correct 'datetime' format. We will utilise the same code as we used before, merely pointing the code to our chosen column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5328e974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert 'date_posted' column to datetime type\n",
    "df['date_posted'] = pd.to_datetime(df['date_posted'], errors='coerce')\n",
    "#  add the .dtypes function to check the data types again  \n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec022313",
   "metadata": {},
   "source": [
    "We can now see that our columns are now correctly reformatted to their correct types. \n",
    "The next logical step is to check to see if there are any duplicate entries in our dataset. For this, we will emply the use of the .duplicated() and .sum() methods to show us a number of duplicate entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffed7b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 2\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (excluding index)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee935d8",
   "metadata": {},
   "source": [
    "Let us check the duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3b9acc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62690</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime       city state country  shape  duration_(seconds)  \\\n",
       "62690 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "70780 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "\n",
       "                 comments date_posted   latitude  longitude  \n",
       "62690   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "70780  2 bright lights...  2013-09-09  38.811944 -77.636667  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show actual duplicate rows\n",
    "df[df.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3e0ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62689</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62690</th>\n",
       "      <td>2013-07-04 22:00:00</td>\n",
       "      <td>shakopee</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Orange fast orbs.</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>44.798056</td>\n",
       "      <td>-93.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70779</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70780</th>\n",
       "      <td>2013-08-30 21:45:00</td>\n",
       "      <td>haymarket</td>\n",
       "      <td>va</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2 bright lights...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>38.811944</td>\n",
       "      <td>-77.636667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime       city state country  shape  duration_(seconds)  \\\n",
       "62689 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "62690 2013-07-04 22:00:00   shakopee    mn      us  light               300.0   \n",
       "70779 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "70780 2013-08-30 21:45:00  haymarket    va      us  light                30.0   \n",
       "\n",
       "                 comments date_posted   latitude  longitude  \n",
       "62689   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "62690   Orange fast orbs.  2013-07-05  44.798056 -93.526667  \n",
       "70779  2 bright lights...  2013-09-09  38.811944 -77.636667  \n",
       "70780  2 bright lights...  2013-09-09  38.811944 -77.636667  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(keep=False)]  # Show all duplicates, including the first occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb03ccd",
   "metadata": {},
   "source": [
    "We can see from our duplicate check, that they have both been duplicated on successive rows after their first entries, and that all information is identically duplicated. We consider it safe therefore, to go ahead and drop these rows from the dataset.\n",
    "For this we will go ahead and employ the *.drop_duplicates function. We'll also make certain to set our argument *inplace=True* to remove them completely. Removing these ensures clean aggregation and avoids skewing any yearly totals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b6a0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeed658",
   "metadata": {},
   "source": [
    "Let us perform a check to ensure that we have removed our 2 duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ade39bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Count total duplicates (excluding index)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b69cb",
   "metadata": {},
   "source": [
    "Excellent, we now have handled our duplicated entries successfully. \n",
    "Next, we think that it would be prudent to standardise values in our key categorical fields. By this we mean, normalise the descriptors into having the same formatting across the entire dataset e.g. 'USA', 'usa' and 'Usa' all mean the same, but could be interpreted as being separate in our analysis. The columns that we want to perform this action against are, *city*, *state*, *country* and *shape*.\n",
    "\n",
    "We will create a 'for loop' to convert our text fields into lowercase, and strip any whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d99d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text fields to lowercase and strip whitespace\n",
    "for col in ['city', 'state', 'country', 'shape']:\n",
    "    df[col] = df[col].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d30095",
   "metadata": {},
   "source": [
    "Let us also quickly check to make sure that our *longitude* and *latitude* values all fall within expected ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04431061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-82.862752 72.7\n",
      "-176.6580556 178.4419\n"
     ]
    }
   ],
   "source": [
    "print(df['latitude'].min(), df['latitude'].max())     # Should be roughly -90 to 90\n",
    "print(df['longitude'].min(), df['longitude'].max())   # Should be roughly -180 to 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd57960",
   "metadata": {},
   "source": [
    "Thankfully, all of our coordinates fall with typical ranges, so there's no further action required here. \n",
    "\n",
    "It was considered at this stage to utilise ChatGPT to create a 'helper function' in order to reverse-geocode the 'unknown' countries in the *country* column of our dataset. Upon further investigation, it was understood that this process would take the GeoPy API many hours to complete, and not without significant risk of potential setbacks.\n",
    "\n",
    "Approximately 12% of the rows in our UFO dataset were missing a recorded country. While it is technically possible to infer country from latitude and longitude via reverse geocoding, this was not implemented at scale due to performance and ethical limitations around API usage.\n",
    "\n",
    "Because this project is primarily concerned with **yearly patterns on a global scale**, the absence of country-level information does not significantly impact the core analysis or hypotheses being tested.\n",
    "\n",
    "This could be considered in a later iteration of our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f51c34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4007eb",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this section we aim to add new features the dataset in order to help us with our analysis.\n",
    "As our analysis goals are primarily concerned with **year-based** analysis, correlation and regression, we must add features that would help support our objectives.\n",
    "\n",
    "We consider our first stage to require adding a *year* column to our dataset, as this will serve as our primary anchor point. \n",
    "Secondly, we will create a column that gives us sightings per year. This will act as our primary dependent variable for regression analysis.\n",
    "\n",
    "We can also consider a count of sightings per country per year, allowing for increased granularity on our Dashboard, later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22004",
   "metadata": {},
   "source": [
    "Firstly, let's go ahead and create a column extracting the *year* from our *datetime* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb81c460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-10-10 20:30:00</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>2004-04-27</td>\n",
       "      <td>29.883056</td>\n",
       "      <td>-97.941111</td>\n",
       "      <td>1949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-10-10 21:00:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>unknown</td>\n",
       "      <td>light</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>2005-12-16</td>\n",
       "      <td>29.384210</td>\n",
       "      <td>-98.581082</td>\n",
       "      <td>1949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955-10-10 17:00:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>2008-01-21</td>\n",
       "      <td>53.200000</td>\n",
       "      <td>-2.916667</td>\n",
       "      <td>1955.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956-10-10 21:00:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20.0</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>2004-01-17</td>\n",
       "      <td>28.978333</td>\n",
       "      <td>-96.645833</td>\n",
       "      <td>1956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-10-10 20:00:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900.0</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>2004-01-22</td>\n",
       "      <td>21.418056</td>\n",
       "      <td>-157.803611</td>\n",
       "      <td>1960.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime                  city    state  country     shape  \\\n",
       "0 1949-10-10 20:30:00            san marcos       tx       us  cylinder   \n",
       "1 1949-10-10 21:00:00          lackland afb       tx  unknown     light   \n",
       "2 1955-10-10 17:00:00  chester (uk/england)  unknown       gb    circle   \n",
       "3 1956-10-10 21:00:00                  edna       tx       us    circle   \n",
       "4 1960-10-10 20:00:00               kaneohe       hi       us     light   \n",
       "\n",
       "   duration_(seconds)                                           comments  \\\n",
       "0              2700.0  This event took place in early fall around 194...   \n",
       "1              7200.0  1949 Lackland AFB&#44 TX.  Lights racing acros...   \n",
       "2                20.0  Green/Orange circular disc over Chester&#44 En...   \n",
       "3                20.0  My older brother and twin sister were leaving ...   \n",
       "4               900.0  AS a Marine 1st Lt. flying an FJ4B fighter/att...   \n",
       "\n",
       "  date_posted   latitude   longitude    year  \n",
       "0  2004-04-27  29.883056  -97.941111  1949.0  \n",
       "1  2005-12-16  29.384210  -98.581082  1949.0  \n",
       "2  2008-01-21  53.200000   -2.916667  1955.0  \n",
       "3  2004-01-17  28.978333  -96.645833  1956.0  \n",
       "4  2004-01-22  21.418056 -157.803611  1960.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column for year\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df.head() # Display the first five rows of the dataframe to check the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e3a516d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "year                         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efc8a2",
   "metadata": {},
   "source": [
    "We can see that our *year* column has been added to our dataset, but the data type clearly displays as a floating point number. We will need to change this to an integer. Before we do this, let us quickly check to see if there are any nulls in our column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a74435c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].isnull().sum() # Check for null values in the 'year' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da33a0",
   "metadata": {},
   "source": [
    "We can see that we have 694 null values in our *year* column (representing ~0.86% of our total data), so let us go ahead and drop these rows. \n",
    "Our decision for this is underpinned by the reason that we are primarily concerned with **year-based** analysis, and focusses on **trends over time**. We also require our *year* field to be valid for the purposes of merging with our *global_stress_events* dataset. \n",
    "\n",
    "Let us proceed then, to remove these unwanted rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abf63cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null values in the 'year' column\n",
    "df = df.dropna(subset=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffeb5d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].isnull().sum()  # Check again for null values in the 'year' column after dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe1fa3",
   "metadata": {},
   "source": [
    "We can now see that we no longer have any NaN values in our *year* column. \n",
    "Let us proceed to convert our *year* data type from *Float64* to *Int*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4ac774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year to integer type\n",
    "df['year'] = df['year'].astype(int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d70eaf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime              datetime64[ns]\n",
       "city                          object\n",
       "state                         object\n",
       "country                       object\n",
       "shape                         object\n",
       "duration_(seconds)           float64\n",
       "comments                      object\n",
       "date_posted           datetime64[ns]\n",
       "latitude                     float64\n",
       "longitude                    float64\n",
       "year                           int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the 'year' column is now of integer type\n",
    "data_types = df.dtypes\n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbab8fd",
   "metadata": {},
   "source": [
    "We have now successfully transformed our *year* column from 'float64' to 'int32' data type. \n",
    "\n",
    "Let us now move on to creating a summary DataFrame for our *sightings_per_year* requirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new summary DataFrame for sightings per year (using ChatGPT's suggestion)\n",
    "sightings_per_year = (\n",
    "    df.groupby('year')\n",
    "    .size()\n",
    "    .reset_index(name='sightings_per_year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf399e4e",
   "metadata": {},
   "source": [
    "Let us quickly check that this procedure has been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9db73ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sightings_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  sightings_per_year\n",
       "0  1906                   1\n",
       "1  1910                   1\n",
       "2  1916                   1\n",
       "3  1920                   1\n",
       "4  1925                   1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows to check the new DataFrame\n",
    "sightings_per_year.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ccb1e",
   "metadata": {},
   "source": [
    "Now that we have extracted our *sightings_per_year* to a new DataFrame, let us move on to handling our global_stress_events dataset. \n",
    "We notice that the years at the head of *sightings_per_year* fall outside of the date range of the global_stress_events data.\n",
    "In order to focus our data, it would seem prudent to align our years across datasets in order to only keep the years relevant to our analysis.\n",
    "Before we move on though, let us export our cleaned data set to our data/clean directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aefced4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned DataFrame to a CSV file\n",
    "df.to_csv(\"../data/clean/ufo_data_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41042887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
