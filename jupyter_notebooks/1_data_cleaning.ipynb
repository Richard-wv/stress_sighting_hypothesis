{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6340765",
   "metadata": {},
   "source": [
    "# **The Stress-Sighting Hypothesis**\n",
    "## A Data-Driven Analysis of Global Events and Reports of the Unknown.\n",
    "\n",
    "**The Stress-Sighting Hypothesis** has the project goal of investigating whether there is a meaningful correlation between the frequency of reported UFO sightings and periods of heightened cultural, political or global stress, using historical event data and publicly reported sightings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54068032",
   "metadata": {},
   "source": [
    "## User Story\n",
    "Alex Holloway is an investigative journalist, known for in-depth features that combine cultural analysis with data storytelling. They work with both independent media outlets and major publishers, seeking to explore how society processes uncertainty — from political unrest to media myths.\n",
    "\n",
    "Alex is planning to write an article on how Global Stress Events impact the number of UFO sightings, and has asked us to conduct our analysis, using the publicly available NUFORC (National UFO Reporting Centre) UFO Sightings dataset found [here](https://www.kaggle.com/datasets/NUFORC/ufo-sightings/data)\n",
    "\n",
    "## Business Requirements\n",
    "In an era shaped by information saturation, political polarisation, and global crises, public perception is increasingly complex and emotionally charged. For journalists, researchers, and communicators, understanding how people respond to uncertainty is as important as the events themselves.\n",
    "\n",
    "This project explores the potential relationship between **reported UFO sightings** and **global stress events**, not to investigate extraterrestrial phenomena, but to examine whether these sightings reflect **underlying patterns of public anxiety, media influence, and cultural tension.**\n",
    "\n",
    "The outcome is a data-driven dashboard designed to support those working at the intersection of **data**, **storytelling**, and **public insight**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bd970",
   "metadata": {},
   "source": [
    "![Alex Holloway – Persona Card](../images/alex_holloway_persona_card.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1b80e",
   "metadata": {},
   "source": [
    "### Alex's Requirements:\n",
    "\n",
    "- **Reveal Patterns**\n",
    "\n",
    "Alex needs to identify correlations between historical periods of stress and spikes in UFO reporting - fast, clearly and without technical issues. \n",
    "\n",
    "- **Narrative Context**\n",
    "\n",
    "They want to explore not just *when* things happened, but *why it matters.* Explanatory text and annotations support deeper storytelling.\n",
    "\n",
    "- **Usable Insights**\n",
    "\n",
    "Our charts and summaries must be easy to extract for use in articles or reports, including explanatory captions and legends.\n",
    "\n",
    "- **Trustworthy Structure**\n",
    "\n",
    "The data pipeline must be transparent, ethical and well-documented to ensure and maintain credibility in their journalistic work.\n",
    "\n",
    "### Value Proposition:\n",
    "Our Dashboard must empower users like Alex to:\n",
    "- Translate complex data into cultural insight\n",
    "- Frame journalistic stories with empirical evidence\n",
    "- Uncover social signals hiding in unconventional data\n",
    "- Offer the audience a grounded perspective on how fear, media, and uncertainty intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31b00e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d0ae1",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "Our Hypotheses for this project are as follows:\n",
    "\n",
    "### **Hypothesis 1:** \n",
    "\n",
    "**There is a positive correlation between the number of glabal stress events in a given year and the number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 2:**\n",
    "\n",
    "**Years with higher total stress severity scores are associated with a greater number of UFO sightings.**\n",
    "\n",
    "### **Hypothesis 3:**\n",
    "\n",
    "**Cultural media events, (such as the release of UFO-themed films or television series) correspond with noticeable short-term spikes in reported sightings.**\n",
    "\n",
    "For the sake of brevity, we will not outline our validation approaches here, as this will be covered in a seperate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd704a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d992c",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning\n",
    "\n",
    "In this section we will look to extract our data and give consideration to how we will clean it in order to make it effective for analysis. \n",
    "Our first step is to load our first dataset: *ufo_data_scrubbed.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283daf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hidde\\AppData\\Local\\Temp\\ipykernel_8008\\3637515765.py:7: DtypeWarning: Columns (5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries and load dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/ufo_data_scrubbed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a70b6d",
   "metadata": {},
   "source": [
    "Straight away we get a data type warning advising us that columns 5 & 9 have mixed data types. This is less than ideal, and will cause issues further down the line when we attempt to merge, aggregate or model our data. \n",
    "\n",
    "let's go ahead and check our columns with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bd4c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime',\n",
       " 'city',\n",
       " 'state',\n",
       " 'country',\n",
       " 'shape',\n",
       " 'duration (seconds)',\n",
       " 'duration (hours/min)',\n",
       " 'comments',\n",
       " 'date posted',\n",
       " 'latitude',\n",
       " 'longitude ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list() # list all columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c5431",
   "metadata": {},
   "source": [
    "Based on the principle of zero-indexing, we can see that our 'duration (seconds)' and 'latitude' columns are likely to be our offenders here. \n",
    "I'll now consult with ChatGPT to suggest code to help identify the problems in our code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92024b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "['2`' '8`' '0.5`']\n",
      "Non-numeric values in latitude:\n",
      "['33q.200088']\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fce55",
   "metadata": {},
   "source": [
    "We can see from the code output that we have some uexpected, non-numeric characters populating several rows. \n",
    "Let's now convert these columns to strictly numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45867502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration (seconds)'] = pd.to_numeric(df['duration (seconds)'], errors='coerce')\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "# coerce will convert non-numeric values to NaN\n",
    "# Code provided by ChatGPT to convert columns to numeric types, handling non-numeric values by converting them to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c65f5",
   "metadata": {},
   "source": [
    "Now, let's run our Helper Function again to check that things have been resolved as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef2deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in duration (seconds):\n",
      "[nan]\n",
      "Non-numeric values in latitude:\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to check if value is numeric after cleaning\n",
    "def is_clean_numeric(value):\n",
    "    value = str(value).strip().lower()\n",
    "    value = value.replace('’', '').replace('‘', '').replace(\"'\", '').replace('\"', '')\n",
    "    value = value.replace('.', '', 1).replace('-', '', 1)\n",
    "    return value.isdigit()\n",
    "\n",
    "# Check non-numeric values in 'duration (seconds)'\n",
    "non_numeric_duration = df[~df['duration (seconds)'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in duration (seconds):\")\n",
    "print(non_numeric_duration['duration (seconds)'].unique())\n",
    "\n",
    "# Check non-numeric values in 'latitude'\n",
    "non_numeric_latitude = df[~df['latitude'].apply(is_clean_numeric)]\n",
    "print(\"Non-numeric values in latitude:\")\n",
    "print(non_numeric_latitude['latitude'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d2002",
   "metadata": {},
   "source": [
    "We can now see that we have replaced the non-numeric values with the NaN (Not a Number) value.\n",
    "Let us now flag the number or rows to be dropped, and export the dropped rows to a new .csv file for the purposes of auditing and transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39268af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag rows with invalid (non-numeric) duration or latitude\n",
    "df['invalid_duration_or_latitude'] = df[['duration (seconds)', 'latitude']].isnull().any(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37846883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to be dropped: 4\n"
     ]
    }
   ],
   "source": [
    "# Count and optionally save them\n",
    "dropped_rows = df[df['invalid_duration_or_latitude']]\n",
    "print(f\"Number of rows to be dropped: {len(dropped_rows)}\")\n",
    "\n",
    "# Export dropped rows for audit\n",
    "dropped_rows.to_csv(\"../data/dropped_invalid_coordinates_or_duration.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf9d6f",
   "metadata": {},
   "source": [
    "As we can see, there are only 4 rows flagged to be dropped here, which represents ~0.005% of our total data, so let's go ahead and drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b88476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with invalid duration or latitude\n",
    "df = df[~df['invalid_duration_or_latitude']].drop(columns='invalid_duration_or_latitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479475d",
   "metadata": {},
   "source": [
    "Now let us check that our NaN values have been dropped from the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf27cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (seconds)    0\n",
      "latitude              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['duration (seconds)', 'latitude']].isnull().sum())\n",
    "# check that there are no more NaN values in the 'duration (seconds)' and 'latitude' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f0b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27822</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43782</th>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.440663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration (seconds)   latitude\n",
       "27822                 NaN  33.932500\n",
       "35692                 NaN  36.974167\n",
       "43782               180.0        NaN\n",
       "58591                 NaN   4.440663"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few of the previously dropped values\n",
    "dropped_rows[['duration (seconds)', 'latitude']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9042070",
   "metadata": {},
   "source": [
    "Here we can see that we have successfully removed the rows with NaN values, and that, as expected there are only 4 rows removed.\n",
    "These rows represent such a small fraction of the data (~0.005%) that their absence will not introduce bias, distort correlations, or meaningfully affect the outcome of any regression or visual insights. Removing them ensures a cleaner, more reliable dataset without sacrificing representativeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b436d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
